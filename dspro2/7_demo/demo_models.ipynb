{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "import signal\n",
    "import queue\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from mac_notifications import client\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.insert(0, \"../6_deployment\")\n",
    "from geo_model_deployer import GeoModelDeployer\n",
    "from image_data_handler_deploy import DeployImageDataHandler\n",
    "\n",
    "sys.path.insert(0, \"../5_evaluation\")\n",
    "from wandb_downloader import WandbDownloader\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from data_loader import resolve_env_variable\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")\n",
    "# Define where to run\n",
    "env_path = \"../../.env\"\n",
    "if not WANDB_TOKEN and os.path.exists(env_path):\n",
    "    load_dotenv(env_path)\n",
    "    WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "\n",
    "    # Print the name of the GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Print the total and available memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "\n",
    "    # Print other properties\n",
    "    device_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"CUDA Capability: {device_properties.major}.{device_properties.minor}\")\n",
    "    print(f\"Multi-Processor Count: {device_properties.multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkillusions\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=WANDB_TOKEN) if WANDB_TOKEN else wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your own wandb entity if you are not part of our project, add WANDB_ENTITY to your .env.\n",
      "dspro2-predicting-country: Found 3 matching runs for datasize 332786 and base_augmentation.\n",
      "{'seed': 42, 'epochs': 50, 'optimizer': 'adamW', 'batch_size': 400, 'model_name': 'efficientnet_b1', 'mapped_data': False, 'dataset_size': 332786, 'weight_decay': 0.01, 'learning_rate': 0.01, 'predict_regions': False, 'input_image_size': [80, 130], 'data_augmentation': 'base_augmentation', 'different_regions': 4596, 'dataset_identifier': '22a493044dbe99c1d431b9ee4656792efbb09ece4182274670ba5faec505d9cf', 'different_countries': 138, 'predict_coordinates': False}\n"
     ]
    }
   ],
   "source": [
    "entity = resolve_env_variable(\"nlp_ls\", \"WANDB_ENTITY\", True)  # Please provide your own entity\n",
    "if entity == \"nlp_ls\":\n",
    "    print(\"Please provide your own wandb entity if you are not part of our project, add WANDB_ENTITY to your .env.\")\n",
    "\n",
    "predict_coordinates = False\n",
    "predict_regions = False\n",
    "run_id = None  # Only get specific run if needed\n",
    "project = \"dspro2-predicting-region\" if predict_regions else (\"dspro2-predicting-coordinates\" if predict_coordinates else \"dspro2-predicting-country\")\n",
    "metric_name = \"Best Validation Accuracy Top 1\" if not predict_coordinates else \"Best Validation Distance (km)\"\n",
    "data_augmentation = \"base_augmentation\"  # or \"full_augmentation_v2\"\n",
    "datasize = 332786  # Replace with the desired datasize\n",
    "file_names_to_download = [\".pth\", \".json\"]\n",
    "image_size = [80, 130]\n",
    "\n",
    "downloader = WandbDownloader(entity, project, data_augmentation, datasize, image_size)\n",
    "try:\n",
    "    run_data = downloader.get_and_collect_best_runs(metric_name, file_names_to_download, run_id=run_id)\n",
    "except Exception as e:\n",
    "    if entity == \"nlp_ls\":\n",
    "        print(\"Using our wandb entity publicly is not supported, please either provide your own entity, download the files manually or correctly authenticate.\")\n",
    "        raise ConnectionError(\"Using our wandb entity publicly is not supported, please either provide your own entity (add WANDB_ENTITY to your .env), download the files manually or correctly authenticate.\")\n",
    "    raise e\n",
    "\n",
    "print(run_data[\"Best Run 1\"][\"parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and creating data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138 countries.\n",
      "Loaded 4596 regions.\n",
      "Loaded 4596 region middle points.\n",
      "Loaded 3595 region to country index mappings.\n"
     ]
    }
   ],
   "source": [
    "run = None\n",
    "\n",
    "for i in range(min(len(run_data.keys()), 5)):\n",
    "    run = run_data[f\"Best Run {i+1}\"]\n",
    "    if run[\"files\"].get(\"best_model\", None):\n",
    "        break\n",
    "    else:\n",
    "        run = None\n",
    "        print(f\"Run {i+1} does not contain the necessary files. Trying the next run...\")\n",
    "\n",
    "if run is None:\n",
    "    raise Exception(\"No run with the necessary files found.\")\n",
    "\n",
    "augmented_transform = None  # Never used for test data\n",
    "base_transform = transforms.Compose([transforms.Resize((image_size[0], image_size[1])), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Creating Dataloaders with the classes\n",
    "files = run[\"files\"]\n",
    "country_to_index = files.get(\"country_to_index.json\", None)\n",
    "region_to_index = files.get(\"region_to_index.json\", None)\n",
    "region_index_to_middle_point = files.get(\"region_index_to_middle_point.json\", None)\n",
    "region_index_to_country_index = files.get(\"region_index_to_country_index.json\", None)\n",
    "\n",
    "data_handler = DeployImageDataHandler(country_to_index, region_to_index, region_index_to_middle_point, region_index_to_country_index, base_transform, join_to_current_dir=\"../7_demo\")\n",
    "country_to_index = data_handler.country_to_index\n",
    "region_to_index = data_handler.region_to_index\n",
    "region_index_to_middle_point = data_handler.region_index_to_middle_point\n",
    "region_index_to_country_index = data_handler.region_index_to_country_index\n",
    "\n",
    "num_regions = data_handler.num_regions\n",
    "num_countries = data_handler.num_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3 if predict_coordinates else (num_regions if predict_regions else num_countries)\n",
    "\n",
    "if num_classes == 0:\n",
    "    raise ValueError(\"No classes detected. Please check the data.\")\n",
    "\n",
    "geo_model = GeoModelDeployer(num_classes=num_classes, predict_coordinates=predict_coordinates, country_to_index=country_to_index, region_to_index=region_to_index, region_index_to_middle_point=region_index_to_middle_point, region_index_to_country_index=region_index_to_country_index, predict_regions=predict_regions if not predict_coordinates else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = run[\"parameters\"][\"model_name\"]\n",
    "pretrained_weights = run[\"files\"][\"best_model\"]\n",
    "\n",
    "geo_model.prepare(model_type=model_name, model_path=pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_event = threading.Event()\n",
    "notification_queue = queue.Queue()\n",
    "\n",
    "\n",
    "def setup_signal_handling():\n",
    "    def signal_handler(_1, _2):\n",
    "        print(\"Shutdown signal received...\")\n",
    "        shutdown_event.set()\n",
    "\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "\n",
    "def file_event_handler(event):\n",
    "    if not event.is_directory and event.event_type == \"created\":\n",
    "        # Wait for a second to ensure the file is not still being written\n",
    "        time.sleep(1)\n",
    "        threading.Thread(target=predict_from_file, args=(event.src_path,)).start()\n",
    "\n",
    "\n",
    "def monitor_directory(path):\n",
    "    observer = Observer()\n",
    "    handler = FileSystemEventHandler()\n",
    "    handler.on_created = file_event_handler\n",
    "    observer.schedule(handler, path, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while not shutdown_event.is_set():\n",
    "            time.sleep(1)\n",
    "    finally:\n",
    "        observer.stop()\n",
    "        observer.join()\n",
    "        print(\"File observer has been stopped.\")\n",
    "\n",
    "\n",
    "def notify_or_print(message):\n",
    "    if not shutdown_event.is_set():\n",
    "        notification_queue.put(message)\n",
    "\n",
    "\n",
    "def handle_notifications():\n",
    "    while not shutdown_event.is_set() or not notification_queue.empty():\n",
    "        try:\n",
    "            messages = notification_queue.get(timeout=1)\n",
    "            try:\n",
    "                if isinstance(messages, str):\n",
    "                    message = messages\n",
    "                else:\n",
    "                    message = \"\\n\".join(messages)\n",
    "                client.create_notification(title=\"Geoguessr location found\", subtitle=message, snooze_button_str=\"Hide\")\n",
    "            except:\n",
    "                print()\n",
    "                if isinstance(messages, str):\n",
    "                    print(message)\n",
    "                else:\n",
    "                    for message in messages:\n",
    "                        print(message)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "\n",
    "def predict_from_file(image_file_path):\n",
    "    try:\n",
    "\n",
    "        if predict_coordinates:\n",
    "            coordinates, cartesian = geo_model.predict_single(data_handler.load_single_image(image_file_path), top_n=5)\n",
    "\n",
    "            notify_or_print(f\"Predicted Coordinates: {coordinates} (Cartesian: {cartesian})\")\n",
    "        else:\n",
    "            if predict_regions:\n",
    "                regions, region_indices, region_probabilities, countries, country_indices, country_probabilities, corresponding_countries, corresponding_country_indices = geo_model.predict_single(data_handler.load_single_image(image_file_path), top_n=3)\n",
    "\n",
    "                # Print the top 3 regions\n",
    "                messages = []\n",
    "                for i, (region, _1, region_probability, corresponding_country, _2) in enumerate(zip(regions, region_indices, region_probabilities, corresponding_countries, corresponding_country_indices)):\n",
    "                    messages.append(f\"Region {i+1}: {region} with prob.: {region_probability*100:.3f}, in: {corresponding_country}\")\n",
    "                notify_or_print(messages)\n",
    "            else:\n",
    "                countries, country_indices, country_probabilities = geo_model.predict_single(data_handler.load_single_image(image_file_path), top_n=3)\n",
    "\n",
    "            # Print the top 3 countries\n",
    "            messages = []\n",
    "            for i, (country, _, country_probability) in enumerate(zip(countries, country_indices, country_probabilities)):\n",
    "                messages.append(f\"Country {i+1}: {country} with prob.: {country_probability*100:.2f}\")\n",
    "            notify_or_print(messages)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while predicting the location of the image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring directory: .data\n"
     ]
    }
   ],
   "source": [
    "# Watch this directory for new files\n",
    "path = \"./.data\"\n",
    "\n",
    "path = data_handler.path_from_current_dir(path)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "print(f\"Monitoring directory: {os.path.basename(path)}\")\n",
    "threading.Thread(target=monitor_directory, args=(path,)).start()\n",
    "handle_notifications()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
