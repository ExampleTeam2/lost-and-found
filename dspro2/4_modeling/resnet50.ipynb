{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import bottleneck\n",
    "from resnet_pytorch import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of processing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 19 files.\n"
     ]
    }
   ],
   "source": [
    "# loading labels\n",
    "files = glob.glob('../1_data_collection/.data/*.json')\n",
    "number_of_files = 20\n",
    "\n",
    "labels = []  # Initialize a list to store data from all files\n",
    "\n",
    "if not files:\n",
    "    print(\"No files found. Check the directory path.\")\n",
    "else:\n",
    "    for file_path in files[:number_of_files]:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                # Load the content of the current file\n",
    "                file_data = json.load(file)\n",
    "                if \"coordinates\" in file_data:\n",
    "                    labels.append(file_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening or reading {file_path}: {e}\")\n",
    "\n",
    "# Output how many files were successfully loaded\n",
    "print(f\"Loaded data from {len(labels)} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'coordinates': [25.727112, 30.561267], 'duration': 10.427},\n",
       " {'coordinates': [35.596395, 64.120085], 'duration': 9.881},\n",
       " {'coordinates': [48.02805, 32.543271], 'duration': 10.303},\n",
       " {'coordinates': [44.324729, 2.441296], 'duration': 10.366},\n",
       " {'coordinates': [9.940982, 33.602745], 'duration': 10.401},\n",
       " {'coordinates': [47.887935, -27.093841], 'duration': 10.309},\n",
       " {'coordinates': [34.571072, 31.374634], 'duration': 9.925},\n",
       " {'coordinates': [-12.265574, 27.792694], 'duration': 9.905},\n",
       " {'coordinates': [30.621082, 22.407837], 'duration': 9.81},\n",
       " {'coordinates': [28.444196, 30.43578], 'duration': 10.419},\n",
       " {'coordinates': [27.150664, -19.849945], 'duration': 10.317},\n",
       " {'coordinates': [8.535778, -59.270355], 'duration': 10.068},\n",
       " {'coordinates': [33.491815, 20.167575], 'duration': 10.122},\n",
       " {'coordinates': [20.132692, -72.791385], 'duration': 10.217},\n",
       " {'coordinates': [17.895419, -11.1704], 'duration': 9.982},\n",
       " {'coordinates': [28.949394, 18.223732], 'duration': 10.443},\n",
       " {'coordinates': [28.719311, 11.100915], 'duration': 10.264},\n",
       " {'coordinates': [-5.609816, 60.492768], 'duration': 10.369},\n",
       " {'coordinates': [10.23696, -28.960029], 'duration': 10.434}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.727112, 30.561267]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just load the first image, avoid Multiplayer\n",
    "labels[0][\"coordinates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of processing input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 image files.\n"
     ]
    }
   ],
   "source": [
    "# loading images\n",
    "files = glob.glob('../1_data_collection/.data/*.png')\n",
    "\n",
    "if not files:\n",
    "    print(\"No files found. Check the directory path.\")\n",
    "else:\n",
    "    images = [] \n",
    "    for file_path in files[:number_of_files]:\n",
    "        try:\n",
    "            # channels, height, width is the pytorch convention\n",
    "            with Image.open(file_path) as img:\n",
    "                img_array = np.array(img)\n",
    "                img_array = img_array.transpose((2, 0, 1))\n",
    "                #images.append(img_array)\n",
    "                images.append(file_path)\n",
    "        except IOError as e:\n",
    "            print(f\"Error opening or reading {file_path}: {e}\")\n",
    "\n",
    "# Output how many files were successfully processed\n",
    "print(f\"Processed {len(images)} image files.\")\n",
    "#display(Image.open(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size (height, width): 180x320\n",
      "Image array shape: (180, 320, 4)\n",
      "Image mode: RGBA\n",
      "Image format: PNG\n"
     ]
    }
   ],
   "source": [
    "# Inside the loop, convert the image to a numpy array and store it\n",
    "with Image.open(file_path) as img:\n",
    "    width, height = img.size\n",
    "    img_array = np.array(img)\n",
    "    print(f\"Image size (height, width): {height}x{width}\")\n",
    "    print(f\"Image array shape: {img_array.shape}\")\n",
    "    print(f\"Image mode: {img.mode}\")\n",
    "    print(f\"Image format: {img.format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../1_data_collection/.data/geoguessr_location_singleplayer_rm8goISVT6SLjGR9_4_resized.png'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images[0].shape\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class of creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split before to train, validation, and test\n",
    "dataset = CustomImageDataset(images, labels, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x136dfe9f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files list from remote\n",
      "Got files list from remote\n",
      "Parsed files list from remote\n",
      "All remote files: 274796\n",
      "Filtering out unpaired files\n",
      "Filtered out 17666 unpaired files\n",
      "Relevant files: 257130\n",
      "Limited files: 200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load\n",
    "\n",
    "\n",
    "list = get_data_to_load(loading_file='./data_list', file_location='../1_data_collection/.data/', image_file_location='../1_data_collection/.data/', allow_new_file_creation=True, from_remote_only=True, download_link='env', limit=100, shuffle_seed=43)\n",
    "\n",
    "# List of data to load\n",
    "print(len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../1_data_collection/.data/geoguessr_result_singleplayer_vDB6RsXTYiw5LgQu_1.json'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_result_singleplayer_vDB6RsXTYiw5LgQu_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m       file_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      3\u001b[0m       label \u001b[38;5;241m=\u001b[39m file_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_result_singleplayer_vDB6RsXTYiw5LgQu_1.json'"
     ]
    }
   ],
   "source": [
    "with open(list[0], 'r') as file:\n",
    "      file_data = json.load(file)\n",
    "      label = file_data['coordinates'] if 'coordinates' in file_data else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, json_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.json_paths = json_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load labels from JSON file\n",
    "        json_path = self.json_paths[idx]\n",
    "        with open(json_path, 'r') as file:\n",
    "            file_data = json.load(file)\n",
    "            label = file_data['coordinates'] if 'coordinates' in file_data else None\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = [item for item in list if item.endswith('.png')]\n",
    "labels = [item for item in list if item.endswith('.json')]\n",
    "\n",
    "assert len(input_images) == len(labels), \"Mismatch in number of images and labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../1_data_collection/.data/geoguessr_result_singleplayer_vDB6RsXTYiw5LgQu_1.json'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(input_images, labels, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_location_singleplayer_oftZ2slpVUv3ipAr_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImages batch shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabels batch shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[60], line 21\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     23\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_location_singleplayer_oftZ2slpVUv3ipAr_0.png'"
     ]
    }
   ],
   "source": [
    "for images, labels in loader:\n",
    "    print(\"Images batch shape:\", images.shape)\n",
    "    print(\"Labels batch shape:\", labels.shape)\n",
    "    break  # After printing the first batch, exit the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_location_singleplayer_3jd33hbXzAHBrYDj_4.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m labels_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path, json_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_images, labels):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load and transform the image\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[1;32m      9\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../1_data_collection/.data/geoguessr_location_singleplayer_3jd33hbXzAHBrYDj_4.png'"
     ]
    }
   ],
   "source": [
    "# Preload images and labels into tensors\n",
    "images = []\n",
    "labels_data = []\n",
    "\n",
    "for img_path, json_path in zip(input_images, labels):\n",
    "    # Load and transform the image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    images.append(image)\n",
    "\n",
    "    # Load and process the JSON file for labels\n",
    "    with open(json_path, 'r') as file:\n",
    "        file_data = json.load(file)\n",
    "        label = file_data['coordinates'] if 'coordinates' in file_data else [0, 0]  # Default to [0,0] if no coordinates\n",
    "        labels_data.append(torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "# Create dataset from preloaded data\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = CustomImageDataset(images, labels_data)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the DataLoader\n",
    "for images, labels in loader:\n",
    "    print(images.shape, labels)\n",
    "    break  # Break after printing first batch for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list of torch.Tensor): List of images as tensors.\n",
    "            labels (list): List of labels.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve image and label already in the correct format\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[0;32m---> 17\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[1;32m     19\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    876\u001b[0m ):\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load images\n",
    "image_paths = glob.glob('../../1_data_collection/.data/*.png')\n",
    "images = []\n",
    "for img_path in image_paths:\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    images.append(image)\n",
    "\n",
    "# Assume labels are loaded and processed into a corresponding list `labels`\n",
    "labels = [...]  # This should match the length and order of `images`\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomImageDataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labels from 20 files.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# Path where the JSON files are stored\n",
    "json_path = '../../1_data_collection/.data/*.json'\n",
    "files = glob.glob(json_path)\n",
    "number_of_files = 20  # Limit the number of files to process\n",
    "\n",
    "labels = []  # Initialize a list to store labels from the files\n",
    "\n",
    "if not files:\n",
    "    print(\"No files found. Check the directory path.\")\n",
    "else:\n",
    "    for file_path in files[:number_of_files]:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                # Load the content of the current file\n",
    "                file_data = json.load(file)\n",
    "                # Check if 'coordinates' key exists\n",
    "                if \"coordinates\" in file_data:\n",
    "                    # Append the coordinates or the whole data, depending on need\n",
    "                    labels.append(file_data['coordinates'])\n",
    "                else:\n",
    "                    # Handle cases where 'coordinates' might not exist\n",
    "                    labels.append(None)  # Or some default value, e.g., {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening or reading {file_path}: {e}\")\n",
    "\n",
    "# Output how many files were successfully loaded with labels\n",
    "print(f\"Loaded labels from {len(labels)} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 14\n",
      "Validation set size: 4\n",
      "Test set size: 2\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "data = {}\n",
    "data[\"input\"] = labels\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.20\n",
    "test_ratio = 0.10\n",
    "\n",
    "# Shuffle data randomly\n",
    "random.shuffle(data[\"input\"])\n",
    "\n",
    "# Calculate split indices\n",
    "total_images = len(data[\"input\"])\n",
    "train_end = int(train_ratio * total_images)\n",
    "val_end = train_end + int(val_ratio * total_images)\n",
    "\n",
    "# Split the data\n",
    "train_data = data[\"input\"][:train_end]\n",
    "val_data = data[\"input\"][train_end:val_end]\n",
    "test_data = data[\"input\"][val_end:]\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataet and crearting, split before!!!\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": val_data,\n",
    "    \"test\": test_data\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: [{'coordinates': [9.940982, 33.602745], 'duration': 10.401}, {'coordinates': [10.23696, -28.960029], 'duration': 10.434}, {'coordinates': [17.895419, -11.1704], 'duration': 9.982}, {'coordinates': [28.444196, 30.43578], 'duration': 10.419}, {'coordinates': [30.621082, 22.407837], 'duration': 9.81}, {'coordinates': [25.727112, 30.561267], 'duration': 10.427}, {'coordinates': [47.887935, -27.093841], 'duration': 10.309}, {'coordinates': [48.02805, 32.543271], 'duration': 10.303}, {'country': 'Tunisia', 'guesses': {'1': {'incorrect': []}, '2': {'incorrect': []}, '3': {'incorrect': []}, '4': {'incorrect': []}, '5': {'incorrect': []}, '6': {'incorrect': []}, '7': {'incorrect': []}, '8': {'incorrect': []}, '9': {'incorrect': []}, '10': {'incorrect': []}, '11': {'incorrect': []}, '12': {'incorrect': ['Türkiye', 'United States']}, '13': {'incorrect': ['Türkiye', 'United States']}, '14': {'incorrect': ['Türkiye', 'United States']}, '15': {'incorrect': ['Türkiye', 'United States']}, '16': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '17': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '18': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '19': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '20': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '21': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '22': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '23': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '24': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '25': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '26': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '27': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '28': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '29': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '30': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '31': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '32': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '33': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '34': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '35': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '36': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '37': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '38': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '39': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '40': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '41': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '42': {'incorrect': ['Türkiye', 'United States', 'Jordan']}, '43': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '44': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '45': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '46': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '47': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '48': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '49': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana']}, '50': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana', 'Argentina']}, '51': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana', 'Argentina']}, '52': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana', 'Argentina']}, '53': {'incorrect': ['Türkiye', 'United States', 'Jordan', 'Botswana', 'Argentina', 'United Arab Emirates']}}, 'people': 8, 'duration': 53.888}, {'coordinates': [34.571072, 31.374634], 'duration': 9.925}, {'coordinates': [-12.265574, 27.792694], 'duration': 9.905}, {'coordinates': [-5.609816, 60.492768], 'duration': 10.369}, {'coordinates': [20.132692, -72.791385], 'duration': 10.217}, {'coordinates': [28.949394, 18.223732], 'duration': 10.443}]\n",
       "    validation: [{'coordinates': [44.324729, 2.441296], 'duration': 10.366}, {'coordinates': [33.491815, 20.167575], 'duration': 10.122}, {'coordinates': [27.150664, -19.849945], 'duration': 10.317}, {'coordinates': [35.596395, 64.120085], 'duration': 9.881}]\n",
       "    test: [{'coordinates': [28.719311, 11.100915], 'duration': 10.264}, {'coordinates': [8.535778, -59.270355], 'duration': 10.068}]\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for resnet18.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet.from_pretrained('resnet18', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet50(10).to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "            \n",
    "print('Training Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
