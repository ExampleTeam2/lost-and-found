{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# important for gpuhub\n",
    "# !pip install -r ../../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "from geo_model_trainer import GeoModelTrainer\n",
    "from image_data_handler import ImageDataHandler\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, hash_filenames\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_TOKEN = os.getenv('WANDB_TOKEN')\n",
    "# Define where to run\n",
    "env_path = '../../.env'\n",
    "if not WANDB_TOKEN and os.path.exists(env_path):\n",
    "  load_dotenv(env_path)\n",
    "  WANDB_TOKEN = os.getenv('WANDB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    \n",
    "    # Print the name of the GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Print the total and available memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "\n",
    "    # Print other properties\n",
    "    device_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"CUDA Capability: {device_properties.major}.{device_properties.minor}\")\n",
    "    print(f\"Multi-Processor Count: {device_properties.multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping remote files check\n",
      "All local files: 705681\n",
      "Relevant files: 705681\n",
      "Limited files: 158000\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 79000 # 100000\n",
    "# Set to False to use non-mapped data (singleplayer distribution), has more data\n",
    "USE_MAPPED = True\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files, zip_load_callback, additional_save_callback = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list_more' if USE_MAPPED else '../3_data_preparation/04_data_cleaning/updated_data_list_non_mapped', \n",
    "                              file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=False, \n",
    "                              from_remote_only=True, download_link='default', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, \n",
    "                              allow_image_file_location_env=True, allow_download_link_env=True, return_zip_load_and_additional_save_callback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79000\n"
     ]
    }
   ],
   "source": [
    "print(len(list_files) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = \"full_augmentation\"\n",
    "\n",
    "# Default was 50, 50\n",
    "image_size = [80, 130]\n",
    "# Original size is  pixelHeight: 180, pixelWidth: 320\n",
    "# image_size = [180, 320]\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "preprocessing_config = { 'data_augmentation': data_augmentation, 'height': image_size[0], 'width': image_size[1], 'train_ratio': train_ratio, 'val_ratio': val_ratio, 'test_ratio': test_ratio }\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "          transforms.Resize((image_size[0], image_size[1])),\n",
    "        ])\n",
    "augmented_transform = None\n",
    "final_transform = transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "if data_augmentation == \"full_augmentation\":\n",
    "    augmented_transform = transforms.Compose([\n",
    "        transforms.RandomPerspective(distortion_scale=0.75, p=0.5),  # Randomly apply perspective transformation\n",
    "        transforms.RandomResizedCrop((image_size[0], image_size[1]), scale=(0.75, 1.0)),  # Randomly crop the image and resize it to the original size\n",
    "        transforms.RandomRotation(10),          # Randomly rotate the image by up to 10 degrees\n",
    "        transforms.ColorJitter(\n",
    "            brightness=(0.5, 1.5),  # Randomly change brightness (lower limit to simulate night, upper limit for bright daylight)\n",
    "            contrast=(0.5, 1.5),    # Randomly change contrast\n",
    "            saturation=(0.5, 1.5),  # Randomly change saturation\n",
    "            hue=(-0.1, 0.1)         # Randomly change hue\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data from: data_79000_data_augmentation=full_augmentationheight=80test_ratio=0.1train_ratio=0.7val_ratio=0.2width=130&3d999c168bedf247468b758771d441ecfd202c97836bed006c225a4afbc8688a.pth\n",
      "Data loaded.\n",
      "Using run link from: run_79000_data_augmentation=full_augmentationheight=80test_ratio=0.1train_ratio=0.7val_ratio=0.2width=130&3d999c168bedf247468b758771d441ecfd202c97836bed006c225a4afbc8688a.wandb\n",
      "Skipping test data saving.\n",
      "Dataset size: 79000\n",
      "Dataset identifier: 3d999c168bedf247468b758771d441ecfd202c97836bed006c225a4afbc8688a\n",
      "Count of different countries: 75\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "\n",
    "# Hash the files list to get a unique identifier for the data\n",
    "hashed_filenames = hash_filenames(list_files)\n",
    "\n",
    "cache = True\n",
    "\n",
    "# Check if the code is running in a notebook\n",
    "running_in_notebook = False\n",
    "try:\n",
    "  get_ipython()\n",
    "  running_in_notebook = True\n",
    "  print(\"Running in a notebook.\")\n",
    "except NameError:\n",
    "  print(\"Running in a script.\")\n",
    "\n",
    "data_handler = ImageDataHandler(list_files, base_transform, augmented_transform, final_transform, preprocessing_config, batch_size=200, train_ratio=train_ratio, val_ratio=val_ratio, test_ratio=test_ratio, cache=cache, cache_zip_load_callback=zip_load_callback, cache_additional_save_callback=additional_save_callback, save_test_data=True, inspect_transformed=running_in_notebook and (data_augmentation == \"full_augmentation\"))\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader\n",
    "country_to_index = data_handler.country_to_index\n",
    "# Path of test data if it should be pushed to wandb\n",
    "test_data_path = data_handler.test_data_path\n",
    "# Previous run id if the test data was already pushed to wandb (to save space)\n",
    "run_link = data_handler.run_link\n",
    "# Path to the run link file to be created if it was not previously (if test data should be pushed)\n",
    "run_link_path = data_handler.run_link_path\n",
    "\n",
    "# Load the country_to_index mapping and print the count of different countries\n",
    "print(\"Dataset size:\", NUMBER_OF_FILES)\n",
    "print(\"Dataset identifier:\", hashed_filenames)\n",
    "print(f\"Count of different countries: {len(country_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 55300 \n",
      "Images batch shape: torch.Size([200, 3, 80, 130])\n",
      "Coordinates batch shape: torch.Size([200, 2])\n",
      "tensor([-31.8840,  26.7955])\n",
      "Country indices: torch.Size([200])\n",
      "tensor(62)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "# Print first batch as an example, to see the structure\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    print(\"Images batch shape:\", images.shape)\n",
    "    print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "    print(coordinates[0])\n",
    "    print(\"Country indices:\", country_indices.shape)\n",
    "    print(country_indices[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkillusions\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ij6hrs1g\n",
      "Sweep URL: https://wandb.ai/killusions/dspro2-predicting-country/sweeps/ij6hrs1g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6l9i2sjc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: full_augmentation\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_identifier: 3d999c168bedf247468b758771d441ecfd202c97836bed006c225a4afbc8688a\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_size: 79000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdifferent_countries: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_image_size: [80, 130]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmapped_data: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: efficientnet_b1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpredict_coordinates: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/linus/gitprojects/dspro2/dspro2/4_modeling/wandb/run-20240601_144752-6l9i2sjc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/killusions/dspro2-predicting-country/runs/6l9i2sjc' target=\"_blank\">zany-sweep-1</a></strong> to <a href='https://wandb.ai/killusions/dspro2-predicting-country' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/killusions/dspro2-predicting-country/sweeps/ij6hrs1g' target=\"_blank\">https://wandb.ai/killusions/dspro2-predicting-country/sweeps/ij6hrs1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/killusions/dspro2-predicting-country' target=\"_blank\">https://wandb.ai/killusions/dspro2-predicting-country</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/killusions/dspro2-predicting-country/sweeps/ij6hrs1g' target=\"_blank\">https://wandb.ai/killusions/dspro2-predicting-country/sweeps/ij6hrs1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/killusions/dspro2-predicting-country/runs/6l9i2sjc' target=\"_blank\">https://wandb.ai/killusions/dspro2-predicting-country/runs/6l9i2sjc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /Users/linus/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
      "100%|██████████| 30.1M/30.1M [00:11<00:00, 2.83MB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: qv3wqfzm\n",
      "Sweep URL: https://wandb.ai/killusions/dspro2-predicting-country/sweeps/qv3wqfzm\n"
     ]
    }
   ],
   "source": [
    "model_types = [\"efficientnet_b1\", \"mobilenet_v2\", \"resnet18\", \"efficientnet_b3\", \"resnet50\"]\n",
    "predict_coordinates=False\n",
    "wandb.login(key=WANDB_TOKEN) if WANDB_TOKEN else wandb.login()\n",
    "\n",
    "for model_type in model_types:\n",
    "    if predict_coordinates:\n",
    "        project_name = \"predicting-coordinates\"\n",
    "        num_classes = 2\n",
    "        sweep_goal = \"minimize\"\n",
    "        sweep_metric_name = \"Validation Distance (km)\"\n",
    "    else:\n",
    "        num_classes = len(country_to_index)\n",
    "        project_name = \"predicting-country\"\n",
    "        sweep_goal = \"maximize\"\n",
    "        sweep_metric_name = \"Validation Accuracy Top 1\"\n",
    "    \n",
    "    sweep_config = {\n",
    "        \"name\": f\"dspro2-basemodel-{model_type}-datasize-{NUMBER_OF_FILES}-input_imagesize-{image_size[0]}x{image_size[1]}\",\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\"goal\": sweep_goal, \"name\": sweep_metric_name},\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\"values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]},\n",
    "            \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "            \"weight_decay\": {\"values\": [1e-1, 1e-2, 1e-3]},\n",
    "            \"epochs\": {\"values\": [50]},\n",
    "            \"dataset_size\": {\"values\": [NUMBER_OF_FILES]},\n",
    "            \"dataset_identifier\": {\"values\": [hashed_filenames]},\n",
    "            \"seed\": {\"values\": [42]},\n",
    "            \"model_name\": {\"values\": [model_type]},\n",
    "            \"input_image_size\": {\"values\": [image_size]},\n",
    "            \"predict_coordinates\": {\"values\": [predict_coordinates]},\n",
    "            \"mapped_data\": {\"values\": [USE_MAPPED]},\n",
    "            \"different_countries\": {\"values\": [len(country_to_index)]},\n",
    "            \"data_augmentation\": {\"values\": [data_augmentation]}\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-{project_name}\")\n",
    "    \n",
    "    def set_run_link(config, run):\n",
    "      global run_link\n",
    "      global run_link_path\n",
    "      if run_link_path is not None:\n",
    "        run_link = run.id\n",
    "        with open(run_link_path, 'w') as f:\n",
    "          f.write(run_link)\n",
    "        # Only write once\n",
    "        run_link_path = None\n",
    "        if additional_save_callback is not None:\n",
    "          additional_save_callback()\n",
    "      elif run_link is not None:\n",
    "        wandb.log({\"test_data_run_id\": run_link})\n",
    "    \n",
    "    trainer = GeoModelTrainer(datasize=NUMBER_OF_FILES, train_dataloader=train_dataloader, val_dataloader=val_dataloader, \n",
    "                              num_classes=num_classes, predict_coordinates=predict_coordinates, country_to_index=country_to_index if not predict_coordinates else None, test_data_path=test_data_path, run_start_callback=set_run_link)\n",
    "\n",
    "    wandb.agent(sweep_id, function=trainer.train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
