{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r ../../requirements.txt wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from resnet_pytorch import ResNet\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, split_json_and_image_files, load_json_files, load_image_files, load_json_file, load_image_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/dspro2/dspro2/.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Count JSON files in the specified directory\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal JSON files in the directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mcount_json_files\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_json_files\u001b[39m(directory):\n\u001b[1;32m      2\u001b[0m     json_file_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      6\u001b[0m             json_file_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def count_json_files(directory):\n",
    "    json_file_count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            json_file_count += 1\n",
    "\n",
    "    return json_file_count\n",
    "\n",
    "# Path to the directory\n",
    "folder_path = \"/home/jovyan/dspro2/dspro2/.data\"\n",
    "\n",
    "# Count JSON files in the specified directory\n",
    "count = count_json_files(folder_path)\n",
    "print(f\"Total JSON files in the directory: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files list from remote\n",
      "Got files list from remote\n",
      "Parsed files list from remote\n",
      "All remote files: 257130\n",
      "All local files: 385096\n",
      "Filtering out unpaired files\n",
      "Filtered out 0 unpaired files\n",
      "Relevant files: 257130\n",
      "Limited files: 200000\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 100000\n",
    "# Set to False to use non-mapped data (singleplayer distribution), has more data\n",
    "USE_MAPPED = False\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list' if USE_MAPPED else '../3_data_preparation/04_data_cleaning/updated_data_list_non_mapped', \n",
    "                              file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=True, \n",
    "                              from_remote_only=True, download_link='env', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, \n",
    "                              allow_image_file_location_env=True)\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(list_files)\n",
    "paired_files = list(zip(json_files, image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Files with no coordinates : 0\n",
      "Missing Files with no country name : 0\n"
     ]
    }
   ],
   "source": [
    "coordinates_count = 0\n",
    "country_names_count = 0\n",
    "for path in json_files:\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if not data.get(\"coordinates\"):\n",
    "            #print(data)\n",
    "            coordinates_count += 1\n",
    "        if not data.get(\"country_name\"):\n",
    "            #print(data)\n",
    "            country_names_count += 1\n",
    "print(f\"Missing Files with no coordinates : {coordinates_count}\")\n",
    "print(f\"Missing Files with no country name : {country_names_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jovyan/dspro2/dspro2/.data/geoguessr_result_singleplayer_00AH3Et9ehwdPYL8_1.json',\n",
       " '/home/jovyan/dspro2/dspro2/.data/geoguessr_location_singleplayer_00AH3Et9ehwdPYL8_1.png')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted image found and skipped: /home/jovyan/dspro2/dspro2/.data/geoguessr_location_singleplayer_6In32jGDkFssGuKR_4.png\n",
      "Corrupted image found and skipped: /home/jovyan/dspro2/dspro2/.data/geoguessr_location_singleplayer_FjbsHTUCjCEjccK3_3.png\n",
      "Total non-corrupted pairs: 99998\n"
     ]
    }
   ],
   "source": [
    "def filter_corrupted_pairs(paired_files):\n",
    "    non_corrupted_pairs = []\n",
    "    \n",
    "    for json_path, image_path in paired_files:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # verify that it's a readable image\n",
    "            non_corrupted_pairs.append((json_path, image_path))\n",
    "        except (IOError, OSError):\n",
    "            print(f\"Corrupted image found and skipped: {image_path}\")\n",
    "\n",
    "    return non_corrupted_pairs\n",
    "\n",
    "# Filter the paired_files list to remove any corrupted entries\n",
    "filtered_paired_files = filter_corrupted_pairs(paired_files)\n",
    "print(f\"Total non-corrupted pairs: {len(filtered_paired_files)}\")\n",
    "\n",
    "def split_json_and_image_files(paired_files):\n",
    "    json_files = [json_file for json_file, _ in paired_files if json_file.endswith('.json')]\n",
    "    image_files = [image_file for _, image_file in paired_files if image_file.endswith('.png')]  # Assuming all images are .png\n",
    "    return json_files, image_files\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(filtered_paired_files)\n",
    "paired_files = filtered_paired_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99998, 99998, 99998)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_files), len(image_files), len(paired_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageNameDataset(Dataset):\n",
    "    def __init__(self, image_paths, json_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.json_paths = json_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_paths[idx], self.json_paths[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(image_files) == len(json_files), \"Mismatch in number of images and labels\"\n",
    "\n",
    "file_name_dataset = CustomImageNameDataset(image_files, json_files, transform=transform)\n",
    "file_name_loader = DataLoader(file_name_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/dspro2/dspro2/.data/geoguessr_location_singleplayer_00AH3Et9ehwdPYL8_1.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_loader.dataset.image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 64\n",
      "Labels: 64\n"
     ]
    }
   ],
   "source": [
    "for temp_image_files, temp_label_files in file_name_loader:\n",
    "    images = load_image_files(temp_image_files)\n",
    "    labels = load_json_files(temp_label_files)\n",
    "    countries = [item['country_name'] for item in labels]\n",
    "    coordinates = [item['coordinates'] for item in labels]\n",
    "    transformed_images = []\n",
    "    for image in images:\n",
    "      transformed_images.append(transform(image))\n",
    "    break  # After the first batch, exit the loop\n",
    "print(\"Images:\", len(images))\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, coordinates, countries):\n",
    "        self.images = images\n",
    "        self.coordinates = coordinates\n",
    "        self.countries = countries\n",
    "        self.country_to_index = {country: idx for idx, country in enumerate(set(countries))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        country_index = self.country_to_index[self.countries[idx]]\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, coordinates, country_index\n",
    "\n",
    "class ImageDataHandler:\n",
    "    def __init__(self, image_paths, json_paths, batch_size=128, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        print(len(image_paths))\n",
    "      \n",
    "        file_name_dataset = CustomImageNameDataset(image_paths, json_paths, transform=transform)\n",
    "        file_name_loader = DataLoader(file_name_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        self.images = []\n",
    "        self.countries = []\n",
    "        self.coordinates = []\n",
    "\n",
    "        for batch_image_files, batch_label_files in file_name_loader:\n",
    "            images = load_image_files(batch_image_files)\n",
    "            labels = load_json_files(batch_label_files)\n",
    "            self.countries.extend([item['country_name'] for item in labels])\n",
    "            #self.countries.extend([item.get('country_name', 'Unknown') for item in labels])\n",
    "            self.coordinates.extend([item['coordinates'] for item in labels])\n",
    "            #self.coordinates.extend([item.get('coordinates', 'Unknown') for item in labels])\n",
    "            for image in images:\n",
    "              self.images.append(transform(image))\n",
    "        \n",
    "        # Initialize datasets and loaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = self.create_loaders(train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "    def create_loaders(self, train_ratio, val_ratio, test_ratio):\n",
    "        assert train_ratio + val_ratio + test_ratio - 1 <= 0.001, \"Ratios should sum to 1\"\n",
    "        \n",
    "        combined = list(zip(self.images, self.coordinates, self.countries))\n",
    "        random.shuffle(combined)\n",
    "        total_count = len(combined)\n",
    "        train_end = int(train_ratio * total_count)\n",
    "        val_end = train_end + int(val_ratio * total_count)\n",
    "\n",
    "        train_data = combined[:train_end]\n",
    "        val_data = combined[train_end:val_end]\n",
    "        test_data = combined[val_end:]\n",
    "        \n",
    "        # Create train, val- and test datasets\n",
    "        train_dataset = CustomImageDataset(*zip(*train_data))\n",
    "        val_dataset = CustomImageDataset(*zip(*val_data))\n",
    "        test_dataset = CustomImageDataset(*zip(*test_data))\n",
    "\n",
    "        # Create train, val- and test dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "data_handler = ImageDataHandler(image_files, json_files)\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f413adde590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 69998 \n",
      "Images batch shape: torch.Size([128, 3, 50, 50])\n",
      "Coordinates batch shape: torch.Size([128, 2])\n",
      "tensor([ 15.9051, 120.7346])\n",
      "Country indices: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "PRINT_FIRST = True\n",
    "\n",
    "# Print first batch as an example, to see the structure\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    if PRINT_FIRST:\n",
    "      print(\"Images batch shape:\", images.shape)\n",
    "      print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "      print(coordinates[0])\n",
    "      print(\"Country indices:\", country_indices.shape)\n",
    "      PRINT_FIRST = False\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ResNet50 model with updated approach\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Change the output features of the last layer to 2 for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "# Initialize the new last layer with random weights\n",
    "nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "nn.init.constant_(model.fc.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_resnet(model_type='resnet50'):\n",
    "    if model_type == 'resnet18':\n",
    "        model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    elif model_type == 'resnet34':\n",
    "        model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "    elif model_type == 'resnet50':\n",
    "        model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    elif model_type == 'resnet101':\n",
    "        model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "    elif model_type == 'resnet152':\n",
    "        model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Supported types are: resnet18, resnet34, resnet50, resnet101, resnet152.\")\n",
    "\n",
    "    # Print the current last layer\n",
    "    print(f\"Original last layer of {model_type}:\", model.fc)\n",
    "    \n",
    "    # Re-initialize the last layer with random weights\n",
    "    model.fc = nn.Linear(model.fc.in_features, model.fc.out_features)\n",
    "    nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "    nn.init.constant_(model.fc.bias, 0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original last layer of resnet18: Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_model = initialize_resnet('resnet18')\n",
    "resnet18_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original last layer of resnet50: Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_model = initialize_resnet('resnet50')\n",
    "resnet50_model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set necessary seeds to make notebook reproducible \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_to_cartesian(lon, lat, R=6371):\n",
    "    # Convert degrees to radians\n",
    "    lon_rad = np.radians(lon)\n",
    "    lat_rad = np.radians(lat)\n",
    "\n",
    "    # Cartesian coordinates using numpy\n",
    "    x = R * np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = R * np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = R * np.sin(lat_rad)\n",
    "    return np.stack([x, y, z], axis=-1)  # ensure the output is a numpy array with the correct shape\n",
    "\n",
    "def spherical_distance(cartesian1, cartesian2, R=6371.0):\n",
    "    cartesian1 = cartesian1.to(cartesian2.device)\n",
    "    dot_product = (cartesian1 * cartesian2).sum(dim=1)\n",
    "    \n",
    "    norms1 = cartesian1.norm(p=2, dim=1)\n",
    "    norms2 = cartesian2.norm(p=2, dim=1)\n",
    "\n",
    "    cos_theta = dot_product / (norms1 * norms2)\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n",
    "    \n",
    "    theta = torch.acos(cos_theta)\n",
    "    # curved distance -> \"Bogenmass\"\n",
    "    distance = R * theta\n",
    "    return distance\n",
    "\n",
    "def mean_spherical_distance(preds, targets):\n",
    "    distances = spherical_distance(preds, targets)\n",
    "    return distances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoModelTrainer:\n",
    "    def __init__(self, num_classes=2, use_coordinates=True):\n",
    "        self.num_classes = num_classes\n",
    "        self.use_coordinates = use_coordinates\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_type = None\n",
    "        self.model = None\n",
    "        \n",
    "    def initialize_model(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        if self.model_type == 'resnet18':\n",
    "            model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet34':\n",
    "            model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet50':\n",
    "            model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet101':\n",
    "            model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet152':\n",
    "            model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Supported types are: resnet18, resnet34, resnet50, resnet101, resnet152.\")\n",
    "        \n",
    "        # Modify the final layer based on the number of classes\n",
    "        model.fc = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(model.fc.bias, 0)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        with wandb.init(reinit=True) as run:\n",
    "            config = run.config\n",
    "            set_seed(config.seed)\n",
    "            \n",
    "            # Set seeds, configure optimizers, losses, etc.\n",
    "            best_val_distance = float('inf')\n",
    "            patience_counter = 0\n",
    "            patience = 30\n",
    "\n",
    "            # Rename run name and initialize parameters in model name\n",
    "            model_name = f\"model_{config.model_name}_lr_{config.learning_rate}_opt_{config.optimizer}_weightDecay_{config.weight_decay}\"\n",
    "            run_name = model_name + f\"_{uuid.uuid4()}\"\n",
    "            wandb.run.name = run_name\n",
    "\n",
    "            # Initialize model, optimizer and criterion\n",
    "            self.model = self.initialize_model(model_type=config.model_name).to(self.device)\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if not n.startswith('fc')], \"lr\": config.learning_rate * 0.1},\n",
    "                {\"params\": self.model.fc.parameters(), \"lr\": config.learning_rate}\n",
    "            ]\n",
    "            optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.weight_decay)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            for epoch in range(config.epochs):\n",
    "                train_loss, train_distance = self.run_epoch(config, criterion, optimizer, is_train=True)\n",
    "                val_loss, val_distance = self.run_epoch(config, criterion, optimizer, is_train=False)\n",
    "                \n",
    "                # Early stopping and logging\n",
    "                if val_distance < best_val_distance:\n",
    "                    best_val_distance = val_distance\n",
    "                    torch.save(model.state_dict(), f\"models/datasize_{NUMBER_OF_FILES}/best_model_checkpoint{model_name}.pth\")\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Stopping early after {patience} epochs without improvement\")\n",
    "                        break\n",
    "\n",
    "                # Log metrics to wandb\n",
    "                wandb.log({\n",
    "                    \"Train Loss\": train_loss,\n",
    "                    \"Train Distance\": train_distance,\n",
    "                    \"Validation Loss\": val_loss,\n",
    "                    \"Validation Distance\": val_distance\n",
    "                })\n",
    "\n",
    "    def run_epoch(self, config, criterion, optimizer, is_train=True):\n",
    "        if is_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_distance = 0.0\n",
    "        data_loader = train_dataloader if is_train else val_dataloader\n",
    "        \n",
    "        for images, coordinates, country_indices in data_loader:\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                images = images.to(self.device)\n",
    "                coordinates = coordinates.to(self.device) if self.use_coordinates else country_indices.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, coordinates)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item() * images.size(0)\n",
    "                total_distance += mean_spherical_distance(outputs, coordinates).item() * images.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        avg_distance = total_distance / len(data_loader.dataset)\n",
    "        return avg_loss, avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluki-st\u001b[0m (\u001b[33mnlp_ls\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 631malxm\n",
      "Sweep URL: https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0xisu2de with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_size: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: resnet50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dspro2/dspro2/4_modeling/wandb/run-20240509_094831-0xisu2de</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de' target=\"_blank\">kind-sweep-1</a></strong> to <a href='https://wandb.ai/nlp_ls/dspro2-basemodel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_types = [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "model_types = [\"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    wandb.login()\n",
    "\n",
    "    sweep_config = {\n",
    "        \"name\": f\"dspro2-basemodel-{model_type}-datasize-{NUMBER_OF_FILES}\",\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\"goal\": \"minimize\", \"name\": \"Validation Distance\"},\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]},\n",
    "            \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "            \"weight_decay\": {\"values\": [1e-3]}, #1e-2, \n",
    "            \"epochs\": {\"values\": [500]},\n",
    "            \"dataset_size\": {\"values\": [NUMBER_OF_FILES]},\n",
    "            \"seed\": {\"values\": [42]},\n",
    "            \"model_name\": {\"values\": [model_type]}\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-basemodel\")\n",
    "    trainer = GeoModelTrainer(num_classes=2, use_coordinates=True)\n",
    "    wandb.agent(sweep_id, function=trainer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
