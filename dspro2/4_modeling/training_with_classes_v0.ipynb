{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r ../../requirements.txt wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from resnet_pytorch import ResNet\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from custom_image_dataset import CustomImageDataset\n",
    "from custom_image_name_dataset import CustomImageNameDataset\n",
    "from image_data_handler import ImageDataHandler\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, split_json_and_image_files, load_json_files, load_image_files, load_json_file, load_image_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files list from remote\n",
      "Got files list from remote\n",
      "Parsed files list from remote\n",
      "All remote files: 257130\n",
      "All local files: 385096\n",
      "Filtering out unpaired files\n",
      "Filtered out 0 unpaired files\n",
      "Relevant files: 257130\n",
      "Limited files: 6000\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 3000 # 100000\n",
    "# Set to False to use non-mapped data (singleplayer distribution), has more data\n",
    "USE_MAPPED = True\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list' if USE_MAPPED else '../3_data_preparation/04_data_cleaning/updated_data_list_non_mapped', \n",
    "                              file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=True, \n",
    "                              from_remote_only=True, download_link='env', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, \n",
    "                              allow_image_file_location_env=True)\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(list_files)\n",
    "paired_files = list(zip(json_files, image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-corrupted pairs: 3000\n"
     ]
    }
   ],
   "source": [
    "def filter_corrupted_pairs(paired_files):\n",
    "    non_corrupted_pairs = []\n",
    "    \n",
    "    for json_path, image_path in paired_files:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # verify that it's a readable image\n",
    "            non_corrupted_pairs.append((json_path, image_path))\n",
    "        except (IOError, OSError):\n",
    "            print(f\"Corrupted image found and skipped: {image_path}\")\n",
    "\n",
    "    return non_corrupted_pairs\n",
    "\n",
    "# Filter the paired_files list to remove any corrupted entries\n",
    "filtered_paired_files = filter_corrupted_pairs(paired_files)\n",
    "print(f\"Total non-corrupted pairs: {len(filtered_paired_files)}\")\n",
    "\n",
    "def split_json_and_image_files(paired_files):\n",
    "    json_files = [json_file for json_file, _ in paired_files if json_file.endswith('.json')]\n",
    "    image_files = [image_file for _, image_file in paired_files if image_file.endswith('.png')]  # Assuming all images are .png\n",
    "    return json_files, image_files\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(filtered_paired_files)\n",
    "paired_files = filtered_paired_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000, 3000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_files), len(image_files), len(paired_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "# Creating Dataloasders with the classes\n",
    "data_handler = ImageDataHandler(image_files, json_files, transform)\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader\n",
    "\n",
    "# Load the country_to_index mapping and print the count of different countries\n",
    "with open(\"country_to_index.json\", 'r') as file:\n",
    "  country_to_index = json.load(file)\n",
    "print(f\"Count of different countries: {len(country_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 2100 \n",
      "Images batch shape: torch.Size([128, 3, 50, 50])\n",
      "Coordinates batch shape: torch.Size([128, 2])\n",
      "tensor([7.3782, 3.9229])\n",
      "Country indices: torch.Size([128])\n",
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "PRINT_FIRST = True\n",
    "\n",
    "# Print first batch as an example, to see the structure\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    if PRINT_FIRST:\n",
    "      print(\"Images batch shape:\", images.shape)\n",
    "      print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "      print(coordinates[0])\n",
    "      print(\"Country indices:\", country_indices.shape)\n",
    "      print(country_indices[0])\n",
    "      PRINT_FIRST = False\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/lukasstoeckli/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:14<00:00, 6.95MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ResNet50 model with updated approach\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Change the output features of the last layer to 2 for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "# Initialize the new last layer with random weights\n",
    "nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "nn.init.constant_(model.fc.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set necessary seeds to make notebook reproducible \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_to_cartesian(lon, lat, R=6371):\n",
    "    # Convert degrees to radians\n",
    "    lon_rad = np.radians(lon)\n",
    "    lat_rad = np.radians(lat)\n",
    "\n",
    "    # Cartesian coordinates using numpy\n",
    "    x = R * np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = R * np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = R * np.sin(lat_rad)\n",
    "    return np.stack([x, y, z], axis=-1)  # ensure the output is a numpy array with the correct shape\n",
    "\n",
    "def spherical_distance(cartesian1, cartesian2, R=6371.0):\n",
    "    cartesian1 = cartesian1.to(cartesian2.device)\n",
    "    dot_product = (cartesian1 * cartesian2).sum(dim=1)\n",
    "    \n",
    "    norms1 = cartesian1.norm(p=2, dim=1)\n",
    "    norms2 = cartesian2.norm(p=2, dim=1)\n",
    "\n",
    "    cos_theta = dot_product / (norms1 * norms2)\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n",
    "    \n",
    "    theta = torch.acos(cos_theta)\n",
    "    # curved distance -> \"Bogenmass\"\n",
    "    distance = R * theta\n",
    "    return distance\n",
    "\n",
    "def mean_spherical_distance(preds, targets):\n",
    "    distances = spherical_distance(preds, targets)\n",
    "    return distances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoModelTrainer:\n",
    "    def __init__(self, num_classes=2, use_coordinates=True):\n",
    "        self.num_classes = num_classes\n",
    "        self.use_coordinates = use_coordinates\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_type = None\n",
    "        self.model = None\n",
    "        \n",
    "    def initialize_model(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        if self.model_type == 'resnet18':\n",
    "            model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet34':\n",
    "            model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet50':\n",
    "            model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet101':\n",
    "            model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        elif self.model_type == 'resnet152':\n",
    "            model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Supported types are: resnet18, resnet34, resnet50, resnet101, resnet152.\")\n",
    "        \n",
    "        # Modify the final layer based on the number of classes\n",
    "        model.fc = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(model.fc.bias, 0)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        with wandb.init(reinit=True) as run:\n",
    "            config = run.config\n",
    "            set_seed(config.seed)\n",
    "            \n",
    "            # Set seeds, configure optimizers, losses, etc.\n",
    "            best_val_distance = float('inf')\n",
    "            patience_counter = 0\n",
    "            patience = 30\n",
    "\n",
    "            # Rename run name and initialize parameters in model name\n",
    "            model_name = f\"model_{config.model_name}_lr_{config.learning_rate}_opt_{config.optimizer}_weightDecay_{config.weight_decay}\"\n",
    "            run_name = model_name + f\"_{uuid.uuid4()}\"\n",
    "            wandb.run.name = run_name\n",
    "\n",
    "            # Initialize model, optimizer and criterion\n",
    "            self.model = self.initialize_model(model_type=config.model_name).to(self.device)\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if not n.startswith('fc')], \"lr\": config.learning_rate * 0.1},\n",
    "                {\"params\": self.model.fc.parameters(), \"lr\": config.learning_rate}\n",
    "            ]\n",
    "            optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.weight_decay)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            for epoch in range(config.epochs):\n",
    "                train_loss, train_distance = self.run_epoch(config, criterion, optimizer, is_train=True)\n",
    "                val_loss, val_distance = self.run_epoch(config, criterion, optimizer, is_train=False)\n",
    "                \n",
    "                # Early stopping and logging\n",
    "                if val_distance < best_val_distance:\n",
    "                    best_val_distance = val_distance\n",
    "                    torch.save(model.state_dict(), f\"models/datasize_{NUMBER_OF_FILES}/best_model_checkpoint{model_name}.pth\")\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Stopping early after {patience} epochs without improvement\")\n",
    "                        break\n",
    "\n",
    "                # Log metrics to wandb\n",
    "                wandb.log({\n",
    "                    \"Train Loss\": train_loss,\n",
    "                    \"Train Distance\": train_distance,\n",
    "                    \"Validation Loss\": val_loss,\n",
    "                    \"Validation Distance\": val_distance\n",
    "                })\n",
    "\n",
    "    def run_epoch(self, config, criterion, optimizer, is_train=True):\n",
    "        if is_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_distance = 0.0\n",
    "        data_loader = train_dataloader if is_train else val_dataloader\n",
    "        \n",
    "        for images, coordinates, country_indices in data_loader:\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                images = images.to(self.device)\n",
    "                coordinates = coordinates.to(self.device) if self.use_coordinates else country_indices.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, coordinates)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item() * images.size(0)\n",
    "                total_distance += mean_spherical_distance(outputs, coordinates).item() * images.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        avg_distance = total_distance / len(data_loader.dataset)\n",
    "        return avg_loss, avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluki-st\u001b[0m (\u001b[33mnlp_ls\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 631malxm\n",
      "Sweep URL: https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0xisu2de with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_size: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: resnet50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dspro2/dspro2/4_modeling/wandb/run-20240509_094831-0xisu2de</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de' target=\"_blank\">kind-sweep-1</a></strong> to <a href='https://wandb.ai/nlp_ls/dspro2-basemodel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/sweeps/631malxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel/runs/0xisu2de</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_types = [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "model_types = [\"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    wandb.login()\n",
    "\n",
    "    sweep_config = {\n",
    "        \"name\": f\"dspro2-basemodel-{model_type}-datasize-{NUMBER_OF_FILES}\",\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\"goal\": \"minimize\", \"name\": \"Validation Distance\"},\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]},\n",
    "            \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "            \"weight_decay\": {\"values\": [1e-3]}, #1e-2, \n",
    "            \"epochs\": {\"values\": [500]},\n",
    "            \"dataset_size\": {\"values\": [NUMBER_OF_FILES]},\n",
    "            \"seed\": {\"values\": [42]},\n",
    "            \"model_name\": {\"values\": [model_type]}\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-basemodel\")\n",
    "    trainer = GeoModelTrainer(num_classes=2, use_coordinates=True)\n",
    "    wandb.agent(sweep_id, function=trainer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
