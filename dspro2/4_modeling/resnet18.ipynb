{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from resnet_pytorch import ResNet\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, split_json_and_image_files, load_json_files, load_image_files, load_json_file, load_image_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All local files: 385695\n",
      "Relevant files: 385695\n",
      "Limited files: 30000\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 10000\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list', file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=False, from_remote_only=True, download_link='env', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, allow_image_file_location_env=True)\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(list_files)\n",
    "paired_files = list(zip(json_files, image_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageNameDataset(Dataset):\n",
    "    def __init__(self, image_paths, json_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.json_paths = json_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_paths[idx], self.json_paths[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(image_files) == len(json_files), \"Mismatch in number of images and labels\"\n",
    "\n",
    "file_name_dataset = CustomImageNameDataset(image_files, json_files, transform=transform)\n",
    "file_name_loader = DataLoader(file_name_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/dspro2/.data/geoguessr_location_singleplayer_01DXbeVAxrsJR1Zu_1.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_loader.dataset.image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 64\n",
      "Labels: 64\n"
     ]
    }
   ],
   "source": [
    "for temp_image_files, temp_label_files in file_name_loader:\n",
    "    images = load_image_files(temp_image_files)\n",
    "    labels = load_json_files(temp_label_files)\n",
    "    countries = [item['country_name'] for item in labels]\n",
    "    coordinates = [item['coordinates'] for item in labels]\n",
    "    transformed_images = []\n",
    "    for image in images:\n",
    "      transformed_images.append(transform(image))\n",
    "    break  # After the first batch, exit the loop\n",
    "print(\"Images:\", len(images))\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, coordinates, countries):\n",
    "        self.images = images\n",
    "        self.coordinates = coordinates\n",
    "        self.countries = countries\n",
    "        self.country_to_index = {country: idx for idx, country in enumerate(set(countries))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        country_index = self.country_to_index[self.countries[idx]]\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, coordinates, country_index\n",
    "\n",
    "class ImageDataHandler:\n",
    "    def __init__(self, image_paths, json_paths, batch_size=64, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        print(len(image_paths))\n",
    "      \n",
    "        file_name_dataset = CustomImageNameDataset(image_paths, json_paths, transform=transform)\n",
    "        file_name_loader = DataLoader(file_name_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        self.images = []\n",
    "        self.countries = []\n",
    "        self.coordinates = []\n",
    "\n",
    "        for batch_image_files, batch_label_files in file_name_loader:\n",
    "            images = load_image_files(batch_image_files)\n",
    "            labels = load_json_files(batch_label_files)\n",
    "            self.countries.extend([item['country_name'] for item in labels])\n",
    "            self.coordinates.extend([item['coordinates'] for item in labels])\n",
    "            for image in images:\n",
    "              self.images.append(transform(image))\n",
    "        \n",
    "        # Initialize datasets and loaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = self.create_loaders(train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "    def create_loaders(self, train_ratio, val_ratio, test_ratio):\n",
    "        assert train_ratio + val_ratio + test_ratio - 1 <= 0.001, \"Ratios should sum to 1\"\n",
    "        \n",
    "        combined = list(zip(self.images, self.coordinates, self.countries))\n",
    "        random.shuffle(combined)\n",
    "        total_count = len(combined)\n",
    "        train_end = int(train_ratio * total_count)\n",
    "        val_end = train_end + int(val_ratio * total_count)\n",
    "\n",
    "        train_data = combined[:train_end]\n",
    "        val_data = combined[train_end:val_end]\n",
    "        test_data = combined[val_end:]\n",
    "        \n",
    "        # Create train, val- and test datasets\n",
    "        train_dataset = CustomImageDataset(*zip(*train_data))\n",
    "        val_dataset = CustomImageDataset(*zip(*val_data))\n",
    "        test_dataset = CustomImageDataset(*zip(*test_data))\n",
    "\n",
    "        # Create train, val- and test dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "data_handler = ImageDataHandler(image_files, json_files)\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 7000 \n",
      "Images batch shape: torch.Size([64, 3, 50, 50])\n",
      "Coordinates batch shape: torch.Size([64, 2])\n",
      "tensor(-6.5915)\n",
      "Country indices: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "PRINT_FIRST = True\n",
    "\n",
    "# Print forst batch as an example, to see the structure\n",
    "# 7000 images need 59 sec for processing as information\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    if PRINT_FIRST:\n",
    "      print(\"Images batch shape:\", images.shape)\n",
    "      print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "      print(coordinates[0][0])\n",
    "      print(\"Country indices:\", country_indices.shape)\n",
    "      PRINT_FIRST = False\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for resnet18.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = ResNet.from_pretrained('resnet18', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set necessary seeds to make notebook reproducible \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "def mean_haversine_distance(preds, targets):\n",
    "    total_distance = 0\n",
    "    total = preds.shape[0]\n",
    "    \n",
    "    for i in range(total):\n",
    "        pred_lon, pred_lat = preds[i, 0], preds[i, 1]\n",
    "        true_lon, true_lat = targets[i, 0], targets[i, 1]\n",
    "        distance = haversine_distance(pred_lon, pred_lat, true_lon, true_lat)\n",
    "        total_distance += distance\n",
    "    \n",
    "    return total_distance / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  with wandb.init(reinit=True) as run:\n",
    "    config = run.config\n",
    "    set_seed(config.seed)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Initializing early stopping\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Rename run name and initialize parameters in model name\n",
    "    model_name = f\"lr_{config.learning_rate}_opt_{config.optimizer}_weightDecay_{config.weight_decay}\"\n",
    "    run_name = model_name + f\"_{uuid.uuid4()}\"\n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    model = ResNet.from_pretrained('resnet18', num_classes=2).to(device)\n",
    "    # SGD optimizer with different learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not n.startswith('fc')], \"lr\": config.learning_rate * 0.1},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": config.learning_rate}\n",
    "    ]\n",
    "\n",
    "    # SGD optimizer\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.weight_decay)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = 0.0\n",
    "        train_mhd = 0.0 # Mean Haversine Distance for training\n",
    "        model.train()\n",
    "\n",
    "        for images, coordinates, country_indices in train_dataloader:\n",
    "            #print(\"Images shape:\", images.shape)\n",
    "            #print(\"Coordinates shape:\", coordinates.shape)\n",
    "            images, coordinates = images.to(device), coordinates.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            \n",
    "            if output.shape != coordinates.shape:\n",
    "                raise ValueError(\"Mismatch in output and target shapes\")\n",
    "\n",
    "\n",
    "            loss = criterion(output, coordinates)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            preds = output.cpu().detach().numpy()\n",
    "            targets = coordinates.cpu().numpy()\n",
    "            train_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_mhd /= len(train_dataloader.dataset)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_mhd = 0.0 # Mean Haversine Distance for validation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, coordinates, country_indices in val_dataloader:\n",
    "                images, coordinates = images.to(device), coordinates.to(device)\n",
    "                output = model(images)\n",
    "                loss = criterion(output, coordinates)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                preds = output.cpu().detach().numpy()\n",
    "                targets = coordinates.cpu().numpy()\n",
    "                val_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_mhd /= len(val_dataloader.dataset)\n",
    "\n",
    "        # Print metrics and log them to wandb\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train MHD: {train_mhd:.4f}, Val Loss: {val_loss:.4f}, Val MHD: {val_mhd:.4f}\")\n",
    "        wandb.log({\n",
    "            \"Train Loss (MSELoss)\": train_loss, \n",
    "            \"Train MHD (Mean Haversine Distance)\": train_mhd, \n",
    "            \"Val Loss (MSELoss)\": val_loss, \n",
    "            \"Val MHD (Mean Haversine Distance)\": val_mhd\n",
    "        })\n",
    "\n",
    "        # Saving model and early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
    "          patience_counter = 0 \n",
    "        else:\n",
    "          patience_counter += 1\n",
    "          if patience_counter >= patience:\n",
    "              print(f\"Stopping early after {patience} epochs without improvement\")\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b1w1w9tv\n",
      "Sweep URL: https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4n3xzwl6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/dspro2/4_modeling/wandb/run-20240502_234058-4n3xzwl6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">good-sweep-1</a></strong> to <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for resnet18.\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([24, 2])\n",
      "Coordinates shape: torch.Size([24, 2])\n",
      "Epoch 1: Train Loss: 2900.3863, Train MHD: 6807.6243, Val Loss: 2764.9907, Val MHD: 6407.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "    torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Parent directory models does not exist.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc97f4d7dbec4206ae7f3a168140d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss (MSELoss)</td><td>▁</td></tr><tr><td>Train MHD (Mean Haversine Distance)</td><td>▁</td></tr><tr><td>Val Loss (MSELoss)</td><td>▁</td></tr><tr><td>Val MHD (Mean Haversine Distance)</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss (MSELoss)</td><td>2900.38632</td></tr><tr><td>Train MHD (Mean Haversine Distance)</td><td>6807.62433</td></tr><tr><td>Val Loss (MSELoss)</td><td>2764.99068</td></tr><tr><td>Val MHD (Mean Haversine Distance)</td><td>6407.17313</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-1</strong> at: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6</a><br/> View project at: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240502_234058-4n3xzwl6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4n3xzwl6 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "    torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Parent directory models does not exist.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4n3xzwl6 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     with _open_zipfile_writer(f) as opened_zipfile:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return container(name_or_buffer)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Parent directory models does not exist.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": f\"dspro2-basemodel-resnet18\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval_accuracy\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]},\n",
    "        \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "        \"weight_decay\": {\"values\": [1e-2, 1e-3]},\n",
    "        \"epochs\": {\"values\": [500]},\n",
    "        \"seed\": {\"values\": [42]}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-basemodel-resnet18\")\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
