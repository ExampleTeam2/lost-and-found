{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from resnet_pytorch import ResNet\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, split_json_and_image_files, load_json_files, load_image_files, load_json_file, load_image_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping all checks\n",
      "All local files: 403361\n",
      "Relevant files: 403361\n",
      "Limited files: 20000\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 10000\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list', file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=False, from_remote_only=True, download_link='env', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, allow_image_file_location_env=True)\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(list_files)\n",
    "paired_files = list(zip(json_files, image_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageNameDataset(Dataset):\n",
    "    def __init__(self, image_paths, json_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.json_paths = json_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_paths[idx], self.json_paths[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(image_files) == len(json_files), \"Mismatch in number of images and labels\"\n",
    "\n",
    "file_name_dataset = CustomImageNameDataset(image_files, json_files, transform=transform)\n",
    "file_name_loader = DataLoader(file_name_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LinUSB/combined/geoguessr_location_singleplayer_00riikZrPAPx5TTg_4.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_loader.dataset.image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 64\n",
      "Labels: 64\n"
     ]
    }
   ],
   "source": [
    "for temp_image_files, temp_label_files in file_name_loader:\n",
    "    images = load_image_files(temp_image_files)\n",
    "    labels = load_json_files(temp_label_files)\n",
    "    countries = [item['country_name'] for item in labels]\n",
    "    coordinates = [item['coordinates'] for item in labels]\n",
    "    transformed_images = []\n",
    "    for image in images:\n",
    "      transformed_images.append(transform(image))\n",
    "    break  # After the first batch, exit the loop\n",
    "print(\"Images:\", len(images))\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, coordinates, countries):\n",
    "        self.images = images\n",
    "        self.coordinates = coordinates\n",
    "        self.countries = countries\n",
    "        self.country_to_index = {country: idx for idx, country in enumerate(set(countries))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        country_index = self.country_to_index[self.countries[idx]]\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, coordinates, country_index\n",
    "\n",
    "class ImageDataHandler:\n",
    "    def __init__(self, image_paths, json_paths, batch_size=64, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        print(len(image_paths))\n",
    "      \n",
    "        file_name_dataset = CustomImageNameDataset(image_paths, json_paths, transform=transform)\n",
    "        file_name_loader = DataLoader(file_name_dataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "        \n",
    "        self.images = []\n",
    "        self.countries = []\n",
    "        self.coordinates = []\n",
    "\n",
    "        for batch_image_files, batch_label_files in file_name_loader:\n",
    "            images = load_image_files(batch_image_files)\n",
    "            labels = load_json_files(batch_label_files)\n",
    "            self.countries.extend([item['country_name'] for item in labels])\n",
    "            self.coordinates.extend([item['coordinates'] for item in labels])\n",
    "            for image in images:\n",
    "              self.images.append(transform(image))\n",
    "        \n",
    "        # Initialize datasets and loaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = self.create_loaders(train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "    def create_loaders(self, train_ratio, val_ratio, test_ratio):\n",
    "        assert train_ratio + val_ratio + test_ratio - 1 <= 0.001, \"Ratios should sum to 1\"\n",
    "        \n",
    "        combined = list(zip(self.images, self.coordinates, self.countries))\n",
    "        random.shuffle(combined)\n",
    "        total_count = len(combined)\n",
    "        train_end = int(train_ratio * total_count)\n",
    "        val_end = train_end + int(val_ratio * total_count)\n",
    "\n",
    "        train_data = combined[:train_end]\n",
    "        val_data = combined[train_end:val_end]\n",
    "        test_data = combined[val_end:]\n",
    "        \n",
    "        # Create train, val- and test datasets\n",
    "        train_dataset = CustomImageDataset(*zip(*train_data))\n",
    "        val_dataset = CustomImageDataset(*zip(*val_data))\n",
    "        test_dataset = CustomImageDataset(*zip(*test_data))\n",
    "\n",
    "        # Create train, val- and test dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linus/gitprojects/dspro2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CustomImageNameDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating Dataloasders with the classes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mtrain_loader\n\u001b[1;32m      4\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mval_loader\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mImageDataHandler.__init__\u001b[0;34m(self, image_paths, json_paths, batch_size, train_ratio, val_ratio, test_ratio)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_image_files, batch_label_files \u001b[38;5;129;01min\u001b[39;00m file_name_loader:\n\u001b[1;32m     32\u001b[0m     images \u001b[38;5;241m=\u001b[39m load_image_files(batch_image_files)\n\u001b[1;32m     33\u001b[0m     labels \u001b[38;5;241m=\u001b[39m load_json_files(batch_label_files)\n",
      "File \u001b[0;32m~/gitprojects/dspro2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gitprojects/dspro2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gitprojects/dspro2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "data_handler = ImageDataHandler(image_files, json_files)\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 7000 \n",
      "Images batch shape: torch.Size([64, 3, 50, 50])\n",
      "Coordinates batch shape: torch.Size([64, 2])\n",
      "tensor(-6.5915)\n",
      "Country indices: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "PRINT_FIRST = True\n",
    "\n",
    "# Print forst batch as an example, to see the structure\n",
    "# 7000 images need 59 sec for processing as information\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    if PRINT_FIRST:\n",
    "      print(\"Images batch shape:\", images.shape)\n",
    "      print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "      print(coordinates[0][0])\n",
    "      print(\"Country indices:\", country_indices.shape)\n",
    "      PRINT_FIRST = False\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for resnet18.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = ResNet.from_pretrained('resnet18', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set necessary seeds to make notebook reproducible \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "def mean_haversine_distance(preds, targets):\n",
    "    total_distance = 0\n",
    "    total = preds.shape[0]\n",
    "    \n",
    "    for i in range(total):\n",
    "        pred_lon, pred_lat = preds[i, 0], preds[i, 1]\n",
    "        true_lon, true_lat = targets[i, 0], targets[i, 1]\n",
    "        distance = haversine_distance(pred_lon, pred_lat, true_lon, true_lat)\n",
    "        total_distance += distance\n",
    "    \n",
    "    return total_distance / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  with wandb.init(reinit=True) as run:\n",
    "    config = run.config\n",
    "    set_seed(config.seed)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Initializing early stopping\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Rename run name and initialize parameters in model name\n",
    "    model_name = f\"lr_{config.learning_rate}_opt_{config.optimizer}_weightDecay_{config.weight_decay}\"\n",
    "    run_name = model_name + f\"_{uuid.uuid4()}\"\n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    model = ResNet.from_pretrained('resnet18', num_classes=2).to(device)\n",
    "    # SGD optimizer with different learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not n.startswith('fc')], \"lr\": config.learning_rate * 0.1},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": config.learning_rate}\n",
    "    ]\n",
    "\n",
    "    # SGD optimizer\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.weight_decay)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = 0.0\n",
    "        train_mhd = 0.0 # Mean Haversine Distance for training\n",
    "        model.train()\n",
    "\n",
    "        for images, coordinates, country_indices in train_dataloader:\n",
    "            #print(\"Images shape:\", images.shape)\n",
    "            #print(\"Coordinates shape:\", coordinates.shape)\n",
    "            images, coordinates = images.to(device), coordinates.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            \n",
    "            if output.shape != coordinates.shape:\n",
    "                raise ValueError(\"Mismatch in output and target shapes\")\n",
    "\n",
    "\n",
    "            loss = criterion(output, coordinates)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            preds = output.cpu().detach().numpy()\n",
    "            targets = coordinates.cpu().numpy()\n",
    "            train_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_mhd /= len(train_dataloader.dataset)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_mhd = 0.0 # Mean Haversine Distance for validation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, coordinates, country_indices in val_dataloader:\n",
    "                images, coordinates = images.to(device), coordinates.to(device)\n",
    "                output = model(images)\n",
    "                loss = criterion(output, coordinates)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                preds = output.cpu().detach().numpy()\n",
    "                targets = coordinates.cpu().numpy()\n",
    "                val_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_mhd /= len(val_dataloader.dataset)\n",
    "\n",
    "        # Print metrics and log them to wandb\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train MHD: {train_mhd:.4f}, Val Loss: {val_loss:.4f}, Val MHD: {val_mhd:.4f}\")\n",
    "        wandb.log({\n",
    "            \"Train Loss (MSELoss)\": train_loss, \n",
    "            \"Train MHD (Mean Haversine Distance)\": train_mhd, \n",
    "            \"Val Loss (MSELoss)\": val_loss, \n",
    "            \"Val MHD (Mean Haversine Distance)\": val_mhd\n",
    "        })\n",
    "\n",
    "        # Saving model and early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
    "          patience_counter = 0 \n",
    "        else:\n",
    "          patience_counter += 1\n",
    "          if patience_counter >= patience:\n",
    "              print(f\"Stopping early after {patience} epochs without improvement\")\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b1w1w9tv\n",
      "Sweep URL: https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4n3xzwl6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/dspro2/4_modeling/wandb/run-20240502_234058-4n3xzwl6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">good-sweep-1</a></strong> to <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/sweeps/b1w1w9tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for resnet18.\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([64, 2])\n",
      "Coordinates shape: torch.Size([64, 2])\n",
      "Output shape: torch.Size([24, 2])\n",
      "Coordinates shape: torch.Size([24, 2])\n",
      "Epoch 1: Train Loss: 2900.3863, Train MHD: 6807.6243, Val Loss: 2764.9907, Val MHD: 6407.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "    torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Parent directory models does not exist.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc97f4d7dbec4206ae7f3a168140d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss (MSELoss)</td><td></td></tr><tr><td>Train MHD (Mean Haversine Distance)</td><td></td></tr><tr><td>Val Loss (MSELoss)</td><td></td></tr><tr><td>Val MHD (Mean Haversine Distance)</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss (MSELoss)</td><td>2900.38632</td></tr><tr><td>Train MHD (Mean Haversine Distance)</td><td>6807.62433</td></tr><tr><td>Val Loss (MSELoss)</td><td>2764.99068</td></tr><tr><td>Val MHD (Mean Haversine Distance)</td><td>6407.17313</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-1</strong> at: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18/runs/4n3xzwl6</a><br/> View project at: <a href='https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18' target=\"_blank\">https://wandb.ai/nlp_ls/dspro2-basemodel-resnet18</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240502_234058-4n3xzwl6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4n3xzwl6 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "    torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Parent directory models does not exist.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4n3xzwl6 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/f3/z549264j6mn3xvpzd9tj43cr0000gn/T/ipykernel_35580/2009476553.py\", line 90, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 627, in save\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     with _open_zipfile_writer(f) as opened_zipfile:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 501, in _open_zipfile_writer\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return container(name_or_buffer)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/lukasstoeckli/GitLabProjects/DSPRO2/dspro2/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 472, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Parent directory models does not exist.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": f\"dspro2-basemodel-resnet18\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval_accuracy\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]},\n",
    "        \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "        \"weight_decay\": {\"values\": [1e-2, 1e-3]},\n",
    "        \"epochs\": {\"values\": [500]},\n",
    "        \"seed\": {\"values\": [42]}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-basemodel-resnet18\")\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
