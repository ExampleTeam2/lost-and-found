{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from resnet_pytorch import ResNet\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from data_loader import get_data_to_load, split_json_and_image_files, load_json_files, load_image_files, load_json_file, load_image_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files list from remote\n",
      "Got files list from remote\n",
      "Parsed files list from remote\n",
      "All remote files: 274796\n",
      "All local files: 257130\n",
      "Filtering out unpaired files\n",
      "Filtered out 17666 unpaired files\n",
      "Relevant files: 257130\n",
      "Downloading 68320 files\n",
      "Downloaded 1000 files\n",
      "Downloaded 2000 files\n",
      "Downloaded 3000 files\n",
      "Downloaded 4000 files\n",
      "Downloaded 5000 files\n",
      "Downloaded 6000 files\n",
      "Downloaded 7000 files\n",
      "Downloaded 8000 files\n",
      "Downloaded 9000 files\n",
      "Downloaded 10000 files\n",
      "Downloaded 11000 files\n",
      "Downloaded 12000 files\n",
      "Downloaded 13000 files\n",
      "Downloaded 14000 files\n",
      "Downloaded 15000 files\n",
      "Downloaded 16000 files\n",
      "Downloaded 17000 files\n",
      "Downloaded 18000 files\n",
      "Downloaded 19000 files\n",
      "Downloaded 20000 files\n",
      "Downloaded 21000 files\n",
      "Downloaded 22000 files\n",
      "Downloaded 23000 files\n",
      "Downloaded 24000 files\n",
      "Downloaded 25000 files\n",
      "Downloaded 26000 files\n",
      "Downloaded 27000 files\n",
      "Downloaded 28000 files\n",
      "Downloaded 29000 files\n",
      "Downloaded 30000 files\n",
      "Downloaded 31000 files\n",
      "Downloaded 32000 files\n",
      "Downloaded 33000 files\n",
      "Downloaded 34000 files\n",
      "Downloaded 34160 files\n",
      "Downloaded 35000 files\n",
      "Downloaded 36000 files\n",
      "Downloaded 37000 files\n",
      "Downloaded 38000 files\n",
      "Downloaded 39000 files\n",
      "Downloaded 40000 files\n",
      "Downloaded 41000 files\n",
      "Downloaded 42000 files\n",
      "Downloaded 43000 files\n",
      "Downloaded 44000 files\n",
      "Downloaded 45000 files\n",
      "Downloaded 46000 files\n",
      "Downloaded 47000 files\n",
      "Downloaded 48000 files\n",
      "Downloaded 49000 files\n",
      "Downloaded 50000 files\n",
      "Downloaded 51000 files\n",
      "Downloaded 52000 files\n",
      "Downloaded 53000 files\n",
      "Downloaded 54000 files\n",
      "Downloaded 55000 files\n",
      "Downloaded 56000 files\n",
      "Downloaded 57000 files\n",
      "Downloaded 58000 files\n",
      "Downloaded 59000 files\n",
      "Downloaded 60000 files\n",
      "Downloaded 61000 files\n",
      "Downloaded 62000 files\n",
      "Downloaded 63000 files\n",
      "Downloaded 64000 files\n",
      "Downloaded 65000 files\n",
      "Downloaded 66000 files\n",
      "Downloaded 67000 files\n",
      "Downloaded 68000 files\n",
      "Downloaded 68320 files\n"
     ]
    }
   ],
   "source": [
    "# set number of files to load\n",
    "NUMBER_OF_FILES = 0 #10000\n",
    "\n",
    "USE_MAPPED = True\n",
    "\n",
    "# get list with local data and file paths\n",
    "list_files = get_data_to_load(loading_file='../3_data_preparation/04_data_cleaning/updated_data_list' if USE_MAPPED else '../3_data_preparation/04_data_cleaning/updated_data_list_non_mapped', file_location='../3_data_preparation/01_enriching/.data', image_file_location='../1_data_collection/.data', allow_new_file_creation=False, from_remote_only=True, download_link='default', limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, allow_image_file_location_env=True, allow_download_link_env=True)\n",
    "\n",
    "json_files, image_files = split_json_and_image_files(list_files)\n",
    "paired_files = list(zip(json_files, image_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageNameDataset(Dataset):\n",
    "    def __init__(self, image_paths, json_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.json_paths = json_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_paths[idx], self.json_paths[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(image_files) == len(json_files), \"Mismatch in number of images and labels\"\n",
    "\n",
    "file_name_dataset = CustomImageNameDataset(image_files, json_files, transform=transform)\n",
    "file_name_loader = DataLoader(file_name_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_loader.dataset.image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_image_files, temp_label_files in file_name_loader:\n",
    "    images = load_image_files(temp_image_files)\n",
    "    labels = load_json_files(temp_label_files)\n",
    "    countries = [item['country_name'] for item in labels]\n",
    "    coordinates = [item['coordinates'] for item in labels]\n",
    "    transformed_images = []\n",
    "    for image in images:\n",
    "      transformed_images.append(transform(image))\n",
    "    break  # After the first batch, exit the loop\n",
    "print(\"Images:\", len(images))\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, coordinates, countries):\n",
    "        self.images = images\n",
    "        self.coordinates = coordinates\n",
    "        self.countries = countries\n",
    "        self.country_to_index = {country: idx for idx, country in enumerate(set(countries))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        country_index = self.country_to_index[self.countries[idx]]\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, coordinates, country_index\n",
    "\n",
    "class ImageDataHandler:\n",
    "    def __init__(self, image_paths, json_paths, batch_size=64, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        print(len(image_paths))\n",
    "      \n",
    "        file_name_dataset = CustomImageNameDataset(image_paths, json_paths, transform=transform)\n",
    "        file_name_loader = DataLoader(file_name_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        self.images = []\n",
    "        self.countries = []\n",
    "        self.coordinates = []\n",
    "\n",
    "        for batch_image_files, batch_label_files in file_name_loader:\n",
    "            images = load_image_files(batch_image_files)\n",
    "            labels = load_json_files(batch_label_files)\n",
    "            self.countries.extend([item['country_name'] for item in labels])\n",
    "            self.coordinates.extend([item['coordinates'] for item in labels])\n",
    "            for image in images:\n",
    "              self.images.append(transform(image))\n",
    "        \n",
    "        # Initialize datasets and loaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = self.create_loaders(train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "    def create_loaders(self, train_ratio, val_ratio, test_ratio):\n",
    "        assert train_ratio + val_ratio + test_ratio - 1 <= 0.001, \"Ratios should sum to 1\"\n",
    "        \n",
    "        combined = list(zip(self.images, self.coordinates, self.countries))\n",
    "        random.shuffle(combined)\n",
    "        total_count = len(combined)\n",
    "        train_end = int(train_ratio * total_count)\n",
    "        val_end = train_end + int(val_ratio * total_count)\n",
    "\n",
    "        train_data = combined[:train_end]\n",
    "        val_data = combined[train_end:val_end]\n",
    "        test_data = combined[val_end:]\n",
    "        \n",
    "        # Create train, val- and test datasets\n",
    "        train_dataset = CustomImageDataset(*zip(*train_data))\n",
    "        val_dataset = CustomImageDataset(*zip(*val_data))\n",
    "        test_dataset = CustomImageDataset(*zip(*test_data))\n",
    "\n",
    "        # Create train, val- and test dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "data_handler = ImageDataHandler(image_files, json_files)\n",
    "train_dataloader = data_handler.train_loader\n",
    "val_dataloader = data_handler.val_loader\n",
    "test_dataloader = data_handler.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train batches:\", len(train_dataloader.dataset), \"\")\n",
    "\n",
    "PRINT_FIRST = True\n",
    "\n",
    "# Print forst batch as an example, to see the structure\n",
    "# 7000 images need 59 sec for processing as information\n",
    "for images, coordinates, country_indices in train_dataloader:\n",
    "    if PRINT_FIRST:\n",
    "      print(\"Images batch shape:\", images.shape)\n",
    "      print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "      print(coordinates[0][0])\n",
    "      print(\"Country indices:\", country_indices.shape)\n",
    "      PRINT_FIRST = False\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = ResNet.from_pretrained('resnet18', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set necessary seeds to make notebook reproducible \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "def mean_haversine_distance(preds, targets):\n",
    "    total_distance = 0\n",
    "    total = preds.shape[0]\n",
    "    \n",
    "    for i in range(total):\n",
    "        pred_lon, pred_lat = preds[i, 0], preds[i, 1]\n",
    "        true_lon, true_lat = targets[i, 0], targets[i, 1]\n",
    "        distance = haversine_distance(pred_lon, pred_lat, true_lon, true_lat)\n",
    "        total_distance += distance\n",
    "    \n",
    "    return total_distance / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  with wandb.init(reinit=True) as run:\n",
    "    config = run.config\n",
    "    set_seed(config.seed)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Initializing early stopping\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Rename run name and initialize parameters in model name\n",
    "    model_name = f\"lr_{config.learning_rate}_opt_{config.optimizer}_weightDecay_{config.weight_decay}\"\n",
    "    run_name = model_name + f\"_{uuid.uuid4()}\"\n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    model = ResNet.from_pretrained('resnet18', num_classes=2).to(device)\n",
    "    # SGD optimizer with different learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not n.startswith('fc')], \"lr\": config.learning_rate * 0.1},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": config.learning_rate}\n",
    "    ]\n",
    "\n",
    "    # SGD optimizer\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.weight_decay)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = 0.0\n",
    "        train_mhd = 0.0 # Mean Haversine Distance for training\n",
    "        model.train()\n",
    "\n",
    "        for images, coordinates, country_indices in train_dataloader:\n",
    "            #print(\"Images shape:\", images.shape)\n",
    "            #print(\"Coordinates shape:\", coordinates.shape)\n",
    "            images, coordinates = images.to(device), coordinates.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            \n",
    "            if output.shape != coordinates.shape:\n",
    "                raise ValueError(\"Mismatch in output and target shapes\")\n",
    "\n",
    "\n",
    "            loss = criterion(output, coordinates)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            preds = output.cpu().detach().numpy()\n",
    "            targets = coordinates.cpu().numpy()\n",
    "            train_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_mhd /= len(train_dataloader.dataset)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_mhd = 0.0 # Mean Haversine Distance for validation\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, coordinates, country_indices in val_dataloader:\n",
    "                images, coordinates = images.to(device), coordinates.to(device)\n",
    "                output = model(images)\n",
    "                loss = criterion(output, coordinates)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                preds = output.cpu().detach().numpy()\n",
    "                targets = coordinates.cpu().numpy()\n",
    "                val_mhd += mean_haversine_distance(preds, targets) * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_mhd /= len(val_dataloader.dataset)\n",
    "\n",
    "        # Print metrics and log them to wandb\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train MHD: {train_mhd:.4f}, Val Loss: {val_loss:.4f}, Val MHD: {val_mhd:.4f}\")\n",
    "        wandb.log({\n",
    "            \"Train Loss (MSELoss)\": train_loss, \n",
    "            \"Train MHD (Mean Haversine Distance)\": train_mhd, \n",
    "            \"Val Loss (MSELoss)\": val_loss, \n",
    "            \"Val MHD (Mean Haversine Distance)\": val_mhd\n",
    "        })\n",
    "\n",
    "        # Saving model and early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          torch.save(model.state_dict(), f\"models/resnet18_best_model_checkpoint{model_name}.pth\")\n",
    "          patience_counter = 0 \n",
    "        else:\n",
    "          patience_counter += 1\n",
    "          if patience_counter >= patience:\n",
    "              print(f\"Stopping early after {patience} epochs without improvement\")\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": f\"dspro2-basemodel-resnet18\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval_accuracy\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]},\n",
    "        \"optimizer\": {\"values\": [\"adamW\"]},\n",
    "        \"weight_decay\": {\"values\": [1e-2, 1e-3]},\n",
    "        \"epochs\": {\"values\": [500]},\n",
    "        \"seed\": {\"values\": [42]}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=f\"dspro2-basemodel-resnet18\")\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
