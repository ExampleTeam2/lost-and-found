{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionally evaluate models after training\n",
    "\n",
    "## Warning: Does not use all values from wandb but data from current project state, make sure they are in sync, otherwise use testing_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# important for gpuhub\n",
    "# !pip install -r ../../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "from geo_model_evaluator import GeoModelEvaluator\n",
    "from wandb_downloader import WandbDownloader\n",
    "\n",
    "sys.path.insert(0, \"../4_modeling\")\n",
    "from image_data_handler import ImageDataHandler\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from data_loader import get_data_to_load, hash_filenames\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")\n",
    "# Define where to run\n",
    "env_path = \"../../.env\"\n",
    "if not WANDB_TOKEN and os.path.exists(env_path):\n",
    "    load_dotenv(env_path)\n",
    "    WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "\n",
    "    # Print the name of the GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Print the total and available memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / 1e9  # Convert bytes to GB\n",
    "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "\n",
    "    # Print other properties\n",
    "    device_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"CUDA Capability: {device_properties.major}.{device_properties.minor}\")\n",
    "    print(f\"Multi-Processor Count: {device_properties.multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set runs to update (should be of the same project, type and dataset (augmentation, size, ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkillusions\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "entity = \"nlp_ls\"\n",
    "project = \"dspro2-predicting-region\"\n",
    "run_ids = [\"zdriu52s\", \"upys7lpk\", \"nxtf1y1t\", \"upys7lpk\", \"ocaaj18u\", \"upys7lpk\", \"ocaaj18u\", \"zdriu52s\", \"nxtf1y1t\", \"34oughmz\"]  # should all be of the same project, type and dataset (augmentation, size, ...)\n",
    "\n",
    "wandb.login(key=WANDB_TOKEN) if WANDB_TOKEN else wandb.login()\n",
    "\n",
    "# Get first run to get the config via the API\n",
    "api = wandb.Api()\n",
    "run = api.run(f\"{entity}/{project}/{run_ids[0]}\")\n",
    "config = run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_FILES = config[\"dataset_size\"]\n",
    "USE_MAPPED = config[\"mapped_data\"]\n",
    "\n",
    "image_size = config[\"input_image_size\"]\n",
    "predict_coordinates = config[\"predict_coordinates\"]\n",
    "predict_regions = config[\"predict_regions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on which gpu to run with best settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 400\n",
    "\n",
    "running_device = \"colab_A100\"\n",
    "\n",
    "if running_device == \"colab_T4\":\n",
    "    # Run unmapped images with low image resolution on colab\n",
    "    BATCH_SIZE = 300\n",
    "\n",
    "elif running_device == \"colab_A100\":\n",
    "    # Run mapped images with high image resolution on colab\n",
    "    BATCH_SIZE = 200\n",
    "\n",
    "elif running_device == \"gpuHub\":\n",
    "    # Run unmapped images with low image resolution on gpuHub\n",
    "    BATCH_SIZE = 200\n",
    "\n",
    "elif running_device == \"gpuHub_augmentedv2\":\n",
    "    # Run unmapped images with low image resolution on gpuHub\n",
    "    BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping remote files check\n",
      "All local files: 705681\n",
      "Relevant files: 705681\n",
      "Limited files: 665572\n"
     ]
    }
   ],
   "source": [
    "# get list with local data and file paths\n",
    "list_files, load_callback, additional_save_callback = get_data_to_load(loading_file=\"../3_data_preparation/04_data_cleaning/updated_data_list_more\" if USE_MAPPED else \"../3_data_preparation/04_data_cleaning/updated_data_list_non_mapped\", file_location=\"../3_data_preparation/01_enriching/.data\", image_file_location=\"../1_data_collection/.data\", allow_new_file_creation=False, from_remote_only=True, download_link=\"default\", limit=NUMBER_OF_FILES, shuffle_seed=42, allow_file_location_env=True, allow_json_file_location_env=True, allow_image_file_location_env=True, allow_download_link_env=True, return_load_and_additional_save_callback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332786\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_FILES = len(list_files) // 2\n",
    "print(NUMBER_OF_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_type = \"regions\" if predict_regions else (\"coordinates\" if predict_coordinates else \"countries\")\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "data_augmentation = config[\"data_augmentation\"]\n",
    "\n",
    "preprocessing_config = {\"data_augmentation\": data_augmentation, \"height\": image_size[0], \"width\": image_size[1], \"train_ratio\": train_ratio, \"val_ratio\": val_ratio, \"test_ratio\": test_ratio}\n",
    "\n",
    "augmented_transform = None  # Never used in this script\n",
    "base_transform = transforms.Compose([transforms.Resize((image_size[0], image_size[1])), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataloasders with the classes\n",
    "\n",
    "# Hash the files list to get a unique identifier for the data\n",
    "hashed_filenames = hash_filenames(list_files)\n",
    "\n",
    "data_handler = ImageDataHandler(list_files, augmented_transform, base_transform, preprocessing_config, prediction_type, batch_size=BATCH_SIZE, train_ratio=train_ratio, val_ratio=val_ratio, test_ratio=test_ratio, cache=False, cache_load_callback=load_callback, cache_additional_save_callback=additional_save_callback, save_test_data=False, inspect_transformed=False, move_files=False, get_cache=True)\n",
    "val_dataloader = data_handler.val_loader\n",
    "country_to_index = data_handler.country_to_index\n",
    "region_to_index = data_handler.region_to_index\n",
    "region_index_to_middle_point = data_handler.region_index_to_middle_point\n",
    "region_index_to_country_index = data_handler.region_index_to_country_index\n",
    "\n",
    "# Load the country_to_index mapping and print the count of different countries\n",
    "print(\"Dataset size:\", NUMBER_OF_FILES)\n",
    "print(\"Dataset identifier:\", hashed_filenames)\n",
    "print(f\"Count of different countries: {len(country_to_index)}\")\n",
    "print(f\"Count of different regions: {len(region_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of val batches:\", len(val_dataloader.dataset), \"\")\n",
    "\n",
    "# Print first batch as an example, to see the structure\n",
    "for images, coordinates, country_indices, region_indices in val_dataloader:\n",
    "    print(\"Images batch shape:\", images.shape)\n",
    "    print(\"Coordinates batch shape:\", coordinates.shape)\n",
    "    print(coordinates[0])\n",
    "    print(\"Country indices:\", country_indices.shape)\n",
    "    print(country_indices[0])\n",
    "    print(\"Region handler:\", region_indices.shape)\n",
    "    print(region_indices[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = config[\"model_name\"]\n",
    "\n",
    "if predict_coordinates:\n",
    "    num_classes = 3\n",
    "elif predict_regions:\n",
    "    num_classes = len(region_to_index)\n",
    "else:\n",
    "    num_classes = len(country_to_index)\n",
    "\n",
    "evaluator = GeoModelEvaluator(val_dataloader=val_dataloader, num_classes=num_classes, predict_coordinates=predict_coordinates, country_to_index=country_to_index, region_to_index=region_to_index, region_index_to_middle_point=region_index_to_middle_point, region_index_to_country_index=region_index_to_country_index, predict_regions=predict_regions if not predict_coordinates else None)\n",
    "\n",
    "# For loading model\n",
    "file_names_to_download = [\".pth\"]\n",
    "# Countries from 81k more mapped dataset, keep in sync with evaluating_models.ipynb\n",
    "countries_only = [\"Albania\", \"Argentina\", \"Australia\", \"Austria\", \"Bangladesh\", \"Belgium\", \"Bolivia, Plurinational State of\", \"Botswana\", \"Brazil\", \"Bulgaria\", \"Cambodia\", \"Canada\", \"Chile\", \"Colombia\", \"Croatia\", \"Czechia\", \"Denmark\", \"Dominican Republic\", \"Ecuador\", \"Estonia\", \"Eswatini\", \"Finland\", \"France\", \"Germany\", \"Ghana\", \"Greece\", \"Guatemala\", \"Hungary\", \"India\", \"Indonesia\", \"Ireland\", \"Israel\", \"Italy\", \"Japan\", \"Kenya\", \"Korea, Republic of\", \"Kyrgyzstan\", \"Lao People's Democratic Republic\", \"Latvia\", \"Lesotho\", \"Lithuania\", \"Malaysia\", \"Malta\", \"Mexico\", \"Montenegro\", \"Netherlands\", \"New Zealand\", \"Nigeria\", \"North Macedonia\", \"Norway\", \"Peru\", \"Philippines\", \"Poland\", \"Portugal\", \"Romania\", \"Russian Federation\", \"Rwanda\", \"Senegal\", \"Serbia\", \"Singapore\", \"Slovakia\", \"Slovenia\", \"South Africa\", \"Spain\", \"Sri Lanka\", \"Sweden\", \"Switzerland\", \"Thailand\", \"T\\u00fcrkiye\", \"Uganda\", \"Ukraine\", \"United Arab Emirates\", \"United Kingdom\", \"United States\", \"Uruguay\"]\n",
    "\n",
    "for run_id in run_ids:\n",
    "    api_run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "    with wandb.init(id=run_id, project=project, entity=entity, resume=True) as run:\n",
    "        # new line\n",
    "        print()\n",
    "        print(f\"Evaluating for run {run.name} ({run.id})\")\n",
    "        # new line\n",
    "        print()\n",
    "\n",
    "        run_info = WandbDownloader.get_run_data(api=api, entity=entity, project=project, run=api_run, file_names=[\".pth\"])\n",
    "        # Load the model\n",
    "        evaluator.evaluate(model_type=model_type, model_path=run_info[\"files\"][\"best_model\"], use_balanced_accuracy=True, second_balanced_on_countries_only=countries_only if NUMBER_OF_FILES > 100000 else None, accuracy_per_country=True, median_metric=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
