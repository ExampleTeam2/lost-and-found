{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize image for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## -> This code does not run and is only meant for comparing external datasets, feel free to modify it for it to work. <-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import concurrent\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from countryconverter import convert_coordinates_to_country_names\n",
    "from region_enricher import RegionEnricher\n",
    "\n",
    "region_enricher = RegionEnricher()\n",
    "gdf = region_enricher.load_enriched_geojson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_country(df):\n",
    "    lat_lon_tuples = [(lat, lon) for lat, lon in zip(df[\"lat\"], df[\"lon\"])]\n",
    "    df[\"country_name\"], df[\"country_code\"] = convert_coordinates_to_country_names(lat_lon_tuples)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_region(df):\n",
    "    lat_lon_tuples = [(lat, lon) for lat, lon in zip(df[\"lat\"], df[\"lon\"])]\n",
    "    df[\"regions\"], df[\"is_in_region\"] = region_enricher.get_region_from_points(gdf, lat_lon_tuples)\n",
    "    return df\n",
    "\n",
    "\n",
    "# convert the data to single json file per file, with the following columns: the name of the file is the {ID}.json and convert it to utf-8\n",
    "# example: {\"coordinates\": [45.96841812133789, -66.80126190185547], \"country_name\": \"Canada\", \"country_code\": \"CA\", \"regions\": [[\"Canada_New Brunswick_CAN-684\", \"POINT (-66.37845 46.625711)\"], [\"United States_Maine_USA-3561\", \"POINT (-69.232905 45.388285)\"], [\"Canada_Prince Edward Island_CAN-687\", \"POINT (-63.249697 46.400597)\"], [\"Canada_Nova Scotia_CAN-685\", \"POINT (-63.310821 45.153825)\"], [\"United States_New Hampshire_USA-3538\", \"POINT (-71.57834 43.68971)\"]], \"is_in_region\": \"False\"}\n",
    "def convert_to_json(df, name):\n",
    "    for i, row in df.iterrows():\n",
    "        data = {}\n",
    "        data[\"coordinates\"] = [row[\"lat\"], row[\"lon\"]]\n",
    "        data[\"country_name\"] = row[\"country_name\"]\n",
    "        data[\"country_code\"] = row[\"country_code\"]\n",
    "        data[\"regions\"] = row[\"regions\"]\n",
    "        data[\"is_in_region\"] = row[\"is_in_region\"]\n",
    "        with open(f'E:\\\\dspro2\\\\Paper_data\\\\{name}\\\\jsons\\\\{row[\"ID\"]}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"E:\\\\dspro2\\\\Paper_data\\\\yfcc4k\\\\info.txt\"\n",
    "output_file = \"E:\\\\dspro2\\\\Paper_data\\\\yfcc4k\\\\info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID      lat       lon\n",
      "0  10201275523  43.6497  -79.3656\n",
      "1   7289030198  40.7401  -73.9855\n",
      "2   4572998878  42.3372  -71.0478\n",
      "3   2764520341  45.4394   12.3334\n",
      "4   4224177264  22.2808  114.1720\n"
     ]
    }
   ],
   "source": [
    "# convert txt to csv line by line splitting by space and saving as csv with comma separator columnnames (ID, lat, lon)\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"ID,lat,lon\\n\")\n",
    "        for line in lines:\n",
    "            f.write(line.replace(\" \", \",\"))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# load csv\n",
    "df = pd.read_csv(output_file, sep=\",\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n",
      "Finished checking if points are in regions.  Now finding nearest regions...\n",
      "Finished finding nearest regions.\n",
      "            ID      lat       lon   country_name country_code  \\\n",
      "0  10201275523  43.6497  -79.3656         Canada           CA   \n",
      "1   7289030198  40.7401  -73.9855  United States           US   \n",
      "2   4572998878  42.3372  -71.0478  United States           US   \n",
      "3   2764520341  45.4394   12.3334          Italy           IT   \n",
      "4   4224177264  22.2808  114.1720      Hong Kong           HK   \n",
      "\n",
      "                                             regions  is_in_region  \n",
      "0  [[United States_Pennsylvania_USA-3560, POINT (...          True  \n",
      "1  [[United States_New Jersey_USA-3558, POINT (-7...          True  \n",
      "2  [[United States_Massachusetts_USA-3513, POINT ...          True  \n",
      "3  [[Italy_Venezia_ITA-5461, POINT (12.480684 45....         False  \n",
      "4  [[Hong Kong_Wan Chai_HKG-5154, POINT (114.1772...         False  \n"
     ]
    }
   ],
   "source": [
    "df = add_country(df)\n",
    "df = add_region(df)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(output_file, index=\"ID\", encoding=\"utf-8\")\n",
    "\n",
    "convert_to_json(df, \"yfcc4k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img2gps = \"E:\\\\dspro2\\\\Paper_data\\\\im2gps3ktest\\\\im2gps3k_places365.csv\"\n",
    "output_img2gps = \"E:\\\\dspro2\\\\Paper_data\\\\im2gps3ktest\\\\im2gps3k_places_changed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking if points are in regions.  Now finding nearest regions...\n",
      "Finished finding nearest regions.\n",
      "                                        ID        lat         lon  \\\n",
      "0  1000269685_e60e9cdfb4_1125_78841376@N00  32.325436  -64.764404   \n",
      "1  1000304467_1a75a200b1_1296_78841376@N00  32.325436  -64.764404   \n",
      "2  1001048550_8e4b47d165_1051_78841376@N00  32.325436  -64.764404   \n",
      "3  1005977048_5ccf8b05d3_1201_91728102@N00  29.976052  122.390356   \n",
      "4  1008804117_ce4e6fef8a_1349_97522422@N00  46.478536   30.758714   \n",
      "\n",
      "  country_name country_code  \\\n",
      "0      Bermuda           BM   \n",
      "1      Bermuda           BM   \n",
      "2      Bermuda           BM   \n",
      "3        China           CN   \n",
      "4      Ukraine           UA   \n",
      "\n",
      "                                             regions  is_in_region  \n",
      "0  [[Bermuda_Devonshire_BMU-5136, POINT (-64.7593...         False  \n",
      "1  [[Bermuda_Devonshire_BMU-5136, POINT (-64.7593...         False  \n",
      "2  [[Bermuda_Devonshire_BMU-5136, POINT (-64.7593...         False  \n",
      "3  [[China_Shanghai_CHN-1819, POINT (121.448736 3...         False  \n",
      "4  [[Ukraine_Odessa_UKR-322, POINT (29.857379 46....          True  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_img2gps, sep=\",\", encoding=\"utf-8\")\n",
    "# only get the columns (IMG_ID, LAT, LON)\n",
    "df = df[[\"IMG_ID\", \"LAT\", \"LON\"]]\n",
    "# rename columns to (ID, lat, lon)\n",
    "df.columns = [\"ID\", \"lat\", \"lon\"]\n",
    "\n",
    "# remove .jpg from ID\n",
    "df[\"ID\"] = df[\"ID\"].str.replace(\".jpg\", \"\")\n",
    "\n",
    "# add country and region\n",
    "df = add_country(df)\n",
    "df = add_region(df)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# save as csv\n",
    "df.to_csv(output_img2gps, sep=\",\", index=\"ID\", encoding=\"utf-8\")\n",
    "\n",
    "convert_to_json(df, \"im2gps3ktest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pillow to resize images to 128x128 for training.\n",
    "(images larger than 128x128 will be resized to 128x128, images smaller than will be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risize an image using pillow\n",
    "def resize_image(image, size, image_path):\n",
    "    try:\n",
    "        return image.resize(size)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        if \"image file is truncated\" in e.__repr__() or (e.strerror is not None and \"image file is truncated\" in e.strerror):\n",
    "            print(\"Image file is truncated. Skipping file: \" + os.path.basename(image_path))\n",
    "            return None\n",
    "        raise e\n",
    "    except SyntaxError as e:\n",
    "        print(e)\n",
    "        if \"broken PNG file\" in e.__repr__() or (e.strerror is not None and \"broken PNG file\" in e.strerror):\n",
    "            print(\"Broken PNG file. Skipping file: \" + os.path.basename(image_path))\n",
    "            return None\n",
    "        raise e\n",
    "\n",
    "\n",
    "# save an image using pillow\n",
    "def save_image(image, image_path):\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "# convert an image to grayscale using pillow\n",
    "def convert_to_grayscale(image):\n",
    "    return image.convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_env = True\n",
    "path_to_images = \"E:\\\\dspro2\\\\Paper_data\\\\yfcc4k\\\\yfcc4k\"\n",
    "path_to_processed_images = \"E:\\\\dspro2\\\\Paper_data\\\\yfcc4k\\\\image_processed\"\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536 files\n"
     ]
    }
   ],
   "source": [
    "image_paths = [os.path.join(path_to_images, f) for f in os.listdir(path_to_images) if os.path.isfile(os.path.join(path_to_images, f))]\n",
    "\n",
    "print(f\"{len(image_paths)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first crop the images to the 16:9 aspect ratio\n",
    "def crop_image(image, image_path):\n",
    "    width, height = image.size\n",
    "    if width / height > 16 / 9:\n",
    "        new_width = int(height * 16 / 9)\n",
    "        left = (width - new_width) / 2\n",
    "        right = (width + new_width) / 2\n",
    "        top = 0\n",
    "        bottom = height\n",
    "    else:\n",
    "        new_height = int(width * 9 / 16)\n",
    "        left = 0\n",
    "        right = width\n",
    "        top = (height - new_height) / 2\n",
    "        bottom = (height + new_height) / 2\n",
    "    return image.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the images\n",
    "def process_image(image_path, path_to_processed_image):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = crop_image(image, image_path)\n",
    "        image = resize_image(image, (130, 80), image_path)\n",
    "        save_image(image, os.path.join(path_to_processed_image, os.path.basename(image_path)))\n",
    "    except UnidentifiedImageError as e:\n",
    "        print(f\"Could not identify image file {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the images in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    executor.map(process_image, image_paths, [path_to_processed_images] * len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 files\n"
     ]
    }
   ],
   "source": [
    "path_to_images = \"E:\\\\dspro2\\\\Paper_data\\\\im2gps3ktest\\\\images\"\n",
    "path_to_processed_images = \"E:\\\\dspro2\\\\Paper_data\\\\im2gps3ktest\\\\image_processed\"\n",
    "image_paths = [os.path.join(path_to_images, f) for f in os.listdir(path_to_images) if os.path.isfile(os.path.join(path_to_images, f))]\n",
    "\n",
    "print(f\"{len(image_paths)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# process the images in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    executor.map(process_image, image_paths, [path_to_processed_images] * len(image_paths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
