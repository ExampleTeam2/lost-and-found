{"path":"dspro2/6_deployment/Week 7_ Transformer/Transformer.pdf","text":"Information Technology Transformer NLP Andreas Marfurt 04.04.2024 Motivation BERT 2 https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html BERT image Motivation BERT 3 https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html BERT image PaLM 4https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html PaLM 5https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html ChatGPT 6https://openai.com/blog/chatgpt ChatGPT 7https://openai.com/blog/chatgpt GPT-4 8https://openai.com/research/gpt-4 GPT-4 9https://openai.com/research/gpt-4 DALL·E 3 10https://openai.com/dall-e-3 A 3D render of a coffee mug placed on a window sill during a stormy day. The storm outside the window is reflected in the coffee, with miniature lightning bolts and turbulent waves seen inside the mug. The room is dimly lit, adding to the dramatic atmosphere. Tiny potato kings wearing majestic crowns, sitting on thrones, overseeing their vast potato kingdom filled with potato subjects and potato castles. DALL·E 3 11https://openai.com/dall-e-3 A 3D render of a coffee mug placed on a window sill during a stormy day. The storm outside the window is reflected in the coffee, with miniature lightning bolts and turbulent waves seen inside the mug. The room is dimly lit, adding to the dramatic atmosphere. Tiny potato kings wearing majestic crowns, sitting on thrones, overseeing their vast potato kingdom filled with potato subjects and potato castles. Overview • Transformer idea • Architecture • Self-attention • Cross-attention • Feed-forward network • Query-key-value attention • Dot-product attention • Multi-head attention • Attention variants • Position encoding 12 What’s wrong with RNNs? • Can’t be parallelized b/c of recurrence • Hidden state ℎ𝑡−1 must be computed before ℎ𝑡 • Long-range dependencies hard to learn, even with LSTM/GRU architecture 13 Transformer • Vaswani et al., 2017 • What if we can parallelize computation on the entire sequence? • Idea 1: Use the attention mechanism instead of recurrence to relate words • Idea 2: Use attention from each word in a sequence to all other words → self-attention 14 Computation in the Transformer • One output vector for every input vector 15 𝑤1 𝑤2 𝑤3 𝑤1 ′ 𝑤2 ′ 𝑤3 ′ Transformer Transformer Architecture 16Vaswani et al., 2017 Previously: Attention in RNNs • Attention from the decoder to encoder hidden states • Compute a context vector 𝑐𝑖 for each decoder time step 17 ℎ1 ℎ2 ℎ3 𝑐𝑖 𝛼1 𝛼2 𝛼3 Self-attention in the Encoder • Attention from an input word to all other words in the input • “What is my meaning in the context of my surrounding words?” 18Vaswani et al., 2017 Self-attention in the Encoder • Each word 𝑤𝑖 looks at all other words in the input 19 𝑤1 𝑤2 𝑤3 𝑤1 ′ 𝑤2 ′ 𝑤3 ′ 𝛼1 𝛼2 𝛼3 Direction of arrows in diagram: Direction of attention: 𝑤1 ′ “looks” at 𝑤1, 𝑤2, 𝑤3 Data flow from 𝑤1, 𝑤2, 𝑤3 to 𝑤1 ′ 𝑤’1 = 𝛼1𝑤1 + 𝛼2𝑤2 + 𝛼3𝑤3 Self-attention in the Decoder • During decoding: Words are not allowed to look into the future • Since our task is to generate the next word from the prefix • If your model gets 0 training loss very quickly, check if information from future words leaks to the current word • Still want to train the entire sequence in parallel • Idea: Causal mask • Word 𝑤1 ′ can only see words 𝑤≤1 • Vectorize computation: Write everything as matrix operations to run efficiently on GPUs/TPUs 20Vaswani et al., 2017 Self-attention in the Decoder • Words can’t see the future 21 𝑤1 𝑤2 𝑤3 𝑤1 ′ 𝑤2 ′ 𝑤3 ′ 𝛼1 𝛼2 𝛼3 𝑤1 𝑤2 𝑤3 𝑤1 ′ 𝑤2 ′ 𝑤3 ′ 𝛼1 𝛼2 𝛼3 𝑤1 ′𝑤1 𝑤1 ′𝑤2 𝑤1 ′𝑤3 𝑤2 ′ 𝑤1 𝑤2 ′ 𝑤2 𝑤2 ′ 𝑤3 𝑤3 ′ 𝑤1 𝑤3 ′ 𝑤2 𝑤3 ′ 𝑤3 Self-attention in the Decoder • Causal mask in matrix notation 22 Interaction matrix Causal mask * = 1 0 0 1 1 0 1 1 1 𝑤1 ′𝑤1 0 0 𝑤2 ′ 𝑤1 𝑤2 ′ 𝑤2 0 𝑤3 ′ 𝑤1 𝑤3 ′ 𝑤2 𝑤3 ′ 𝑤3 𝑤1 ′𝑤1 𝑤1 ′𝑤2 𝑤1 ′𝑤3 𝑤2 ′ 𝑤1 𝑤2 ′ 𝑤2 𝑤2 ′ 𝑤3 𝑤3 ′ 𝑤1 𝑤3 ′ 𝑤2 𝑤3 ′ 𝑤3 Self-attention in the Decoder • Causal mask in matrix notation 23 Interaction matrix Causal mask * = 1 0 0 1 1 0 1 1 1 𝑤1 ′𝑤1 0 0 𝑤2 ′ 𝑤1 𝑤2 ′ 𝑤2 0 𝑤3 ′ 𝑤1 𝑤3 ′ 𝑤2 𝑤3 ′ 𝑤3 element-wise multiplication Cross-attention • Attention from the decoder to the encoder • This is what we saw in RNNs as well • We will see later what the two arrows mean that come from the encoder (these are the keys and values in QKV attention) 24Vaswani et al., 2017 Feed-forward Network (FFN) • Applies the following operations: • Linear layer • ReLU (GELU/SwiGLU in modern LLMs) • Linear layer • First linear layer is an up-projection: Projects into higher dimension (512 → 2048 in the paper) • This is called the “inner” dimension of the FFN • Second layer projects down to 512- dimensional vector again 25Vaswani et al., 2017 Role of FFN • Still a topic of research what exactly they add • In general: adding a non-linearity (such as a ReLU) to a network increases the complexity of functions it can implement • Up-projection for the inner dimension: Potentially disentangles some factors of variation such that they can be treated separately by the non-linearity 26 A Note on Layers • What is a layer in this architecture? • Sometimes a Transformer block (attentions + FFN) is called a Transformer layer • And self-attention, cross-attention and FFN are called sublayers • But with the original definition of a layer, just the FFN consists of at least 2 layers… • Make sure people know what you are talking about by giving context • … and ask if you’re unsure! 27Vaswani et al., 2017 Intuition: Skip/Residual Connections • Direct connections around a (sub)layer • Shorter path length → better gradient flow 28Vaswani et al., 2017 Advanced Intuition: (Topological) Spaces • ML researchers/practitioners are often talking about spaces • Embedding space • Latent space • Query/key/value space • Feature space (e.g. vision vs. text in multimodal models) • We can map from one space to the other with the help of a projection • If we map from higher dimensional (e.g. one-hot vector) to lower dimensional (300-dimensional word vectors), we also call the mapping an embedding, i.e. we embed an object (=word) into the 300-dimensional latent/embedding space 29 Advanced Intuition: (Topological) Spaces • Imperfect analogy: RGB vs. hexadecimal “color spaces” • Different coordinate systems for describing colors • Can map from one system to the other • Can perform operations (e.g. addition, increasing hue) on the elements of the same system, but not on elements from two different systems • Same with NN spaces: adding vectors in different spaces is not meaningful • Project to same space, then perform the operation • Lucky for us: we let the model learn this projection 30 Query-Key-Value Attention 31 QKV Attention • New way to look at attention • Attention consists of 3 parts • Queries: formulate questions on new information that we need • Keys: match the queries if they have that type of information • Values: contain the information • Attention function tries to find matching keys for queries → computes attention weights (= how well they match) • Attention weights are multiplied with the values to give us the new information 32 QKV Attention • How does original attention match this view? • Queries: Previous decoder hidden state • Keys: Encoder hidden states • Values: Encoder hidden states 33 Attention Definition • Context vector 𝑐𝑖 • a weighted sum of encoder hidden states • computed for each decoding time step 𝑐𝑖 = ෍ 𝑗=1 𝑛 𝛼𝑖𝑗ℎ𝑗 (enc) • Attention weights 𝛼𝑖𝑗 𝛼𝑖𝑗 = softmax 𝑒𝑖𝑗 = exp 𝑒𝑖𝑗 σ𝑘=1 𝑛 exp 𝑒𝑖𝑘 • Attention function 𝑓att 𝑒𝑖𝑗 = 𝑓att ℎ𝑖−1 (dec), ℎ𝑗 (enc) 34 i: time step in decoder j: time step in encoder previous decoder hidden state current encoder hidden state Attention Definition • Context vector 𝑐𝑖 • a weighted sum of encoder hidden states • computed for each decoding time step 𝑐𝑖 = ෍ 𝑗=1 𝑛 𝛼𝑖𝑗ℎ𝑗 (enc) • Attention weights 𝛼𝑖𝑗 𝛼𝑖𝑗 = softmax 𝑒𝑖𝑗 = exp 𝑒𝑖𝑗 σ𝑘=1 𝑛 exp 𝑒𝑖𝑘 • Attention function 𝑓att 𝑒𝑖𝑗 = 𝑓att ℎ𝑖−1 (dec), ℎ𝑗 (enc) 35 i: time step in decoder j: time step in encoder previous decoder hidden state current encoder hidden state Values Queries Keys QKV Attention • How does original attention match this view? • Queries: Previous decoder hidden state • Keys: Encoder hidden states • Values: Encoder hidden states • New: separate keys and values (through projections) 36 What are the queries, keys and values in Transformer? • Linear projections (𝑊𝑥 + 𝑏) of the input • Different weights & biases for Q, K, V • Self-attention • Inputs to the Transformer block • Cross-attention • Queries: Output of decoder self-attention • Keys & values: Final layer encoder hidden states 37Vaswani et al., 2017 Dot-product Attention • We saw multiplicative attention for RNNs: 𝑓att ℎ𝑖−1 (dec), ℎ𝑗 (enc) = ℎ𝑖−1 (dec)𝑊ℎ𝑗 (enc) • Transformer uses scaled dot-product attention: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖𝑘𝑗 ⊤ 𝑑 𝑑 is the dimension of 𝑞𝑖, 𝑘𝑗 38 𝑞𝑖, 𝑘𝑗 are the output of projections themselves Dot-product Attention • We saw multiplicative attention for RNNs: 𝑓att ℎ𝑖−1 (dec), ℎ𝑗 (enc) = ℎ𝑖−1 (dec)𝑊ℎ𝑗 (enc) • If we write out Transformer scaled dot-product attention and use ℎ𝑖−1 (dec), ℎ𝑗 (enc) as inputs: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖𝑘𝑗 ⊤ 𝑑 = 𝑊𝑞ℎ𝑖−1 (dec) + 𝑏𝑞 𝑊𝑘ℎ𝑗 (enc) + 𝑏𝑘 ⊤ 𝑑−1 2 39 Dot-product Attention • We saw multiplicative attention for RNNs: 𝑓att ℎ𝑖−1 (dec), ℎ𝑗 (enc) = ℎ𝑖−1 (dec)𝑊ℎ𝑗 (enc) • If we write out Transformer scaled dot-product attention and use ℎ𝑖−1 (dec), ℎ𝑗 (enc) as inputs: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖𝑘𝑗 ⊤ 𝑑 = 𝑊𝑞ℎ𝑖−1 (dec) + 𝑏𝑞 𝑊𝑘ℎ𝑗 (enc) + 𝑏𝑘 ⊤ 𝑑−1 2 40 Added 1 projection matrix, 2 biases, 1 scalar Dot-product Attention • Transformer attention function: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖𝑘𝑗 ⊤ 𝑑 • Attention weights (as before): 𝛼𝑖𝑗 = softmax 𝑓att 𝑞𝑖, 𝑘𝑗 • Attention output (previously our context vector): output = ෍ 𝑗=1 𝑛 𝛼𝑖𝑗𝑣𝑗 41 Dot-product Attention We can write all of this in matrix notation: Attention 𝑄, 𝐾, 𝑉 = softmax 𝑄𝐾⊤ 𝑑 𝑉 42Vaswani et al., 2017 Multi-head Attention • Transformer adds 1 more trick: multi-head attention 1. Split the input to the attention function into ℎ chunks of dimension 𝑑/ℎ (ℎ is the number of heads, don’t confuse with RNN hidden states) 2. Compute attention on each chunk separately 3. Concatenate the result 43Vaswani et al., 2017 1 2 3 4 5 6 1 2 3 4 5 6 a b c d e f Attention( ) Attention( ) Attention( ) Multi-head Attention • Transformer adds 1 more trick: multi-head attention 1. Split the input to the attention function into ℎ chunks of dimension 𝑑/ℎ (ℎ is the number of heads, don’t confuse with RNN hidden states) 2. Compute attention on each chunk separately 3. Concatenate the result 44Vaswani et al., 2017 1 2 3 4 5 6 1 2 3 4 5 6 a b c d e f Attention( ) Attention( ) Attention( ) These are called attention heads Attention Variants • Multi-query attention • Shazeer, 2019 • Instead of h heads for keys/values, use only 1 head • Keep h heads for queries • Grouped query attention • Ainslie et al., 2023 • Groups of keys/values (can be chosen to equal the number of GPUs) • Used in modern LLMs for efficiency 45Ainslie et al., 2023 Transformer Attention Output • We no longer use the context vector to predict the next hidden state • Instead, the attention output is our next hidden state • We refine this representation over multiple iterations of self-attention → (cross- attention in the decoder →) FFN • Around each attention and FFN block, there is a residual connection • The input to the block gets added to the output, and layer normalization is applied 46Vaswani et al., 2017 Layer Normalization • Purpose: Keep activations (= hidden states) in a similar range across layers → No explosion/vanishing of activations → No explosion/vanishing of gradients • Goal: Get outputs with 0 mean and unit variance 𝑦 = 𝑥 − E 𝑥 Var 𝑥 + 𝜖 ∗ 𝛾 + 𝛽 𝛾, 𝛽 are learnable parameters 47https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html Layer Normalization • Purpose: Keep activations (= hidden states) in a similar range across layers → No explosion/vanishing of activations → No explosion/vanishing of gradients • Goal: Get outputs with 0 mean and unit variance 𝑦 = 𝑥 − E 𝑥 Var 𝑥 + 𝜖 ∗ 𝛾 + 𝛽 𝛾, 𝛽 are learnable parameters 48https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html I’m not sure these are necessary… query length key length vector dim Complexity of Attention • Every query 𝑞𝑖 looks at every key 𝑘𝑗 • This mean m x n x d multiplications in the dot-product • m = n for self-attention • For long inputs, this can be quite costly… • There are multiple approaches that try to make attention more efficient, e.g. Transformer-XL, Longformer, Performer, … • We won’t talk about them in this class 49 Position Encoding 50 Why do we need position encoding? • RNNs • Process input sequentially • It is clear which word came before and after • Transformers • Process the entire sequence in parallel • When we compare the word at position 𝑖 to that at position 𝑗, how do we know their order and how far apart they are? → Position encoding 51 Types of Position Encoding • Absolute position encoding • As a function of the position • As a learned embedding • Relative position embeddings 52 Absolute Position Encoding • For an input word at position 𝑖, create a 𝑑-dimensional vector • The entries at dimension 𝑗 of this position vector are: pos 𝑖, 2𝑗 = sin 𝑖/10000 2𝑗/𝑑 pos 𝑖, 2𝑗 + 1 = cos 𝑖/100002𝑗/𝑑 53Vaswani et al., 2017 Absolute Position Encoding • For an input word at position 𝑖, create a 𝑑-dimensional vector • The entries at dimension 𝑗 of this position vector are: pos 𝑖, 2𝑗 = sin 𝑖/10000 2𝑗/𝑑 pos 𝑖, 2𝑗 + 1 = cos 𝑖/100002𝑗/𝑑 54Vaswani et al., 2017 0 1 2 3 4 … even odd Absolute Position Embedding • Learn a separate embedding for each position 𝑖 = 1, …, max_pos • max_pos: Maximum input length that was used during training, typically 512 (BERT) or 1024 (BART) • How do we deal with longer inputs? • Truncate input • Copy the position embedding at the last position for each following position • Absolute position embeddings are added to the word embeddings at the input 55Vaswani et al., 2017 Absolute Position Encoding vs. Embedding • Performance the same • Advantage for absolute position encoding: Works for any input length… • … but absolute position embeddings are always used in practice • For standard Transformers, very long inputs are not the typical input • Transformers that are designed to deal with long inputs either train larger position embeddings, or have a different strategy to deal with position 56 Relative Position Embedding (only for self-attention) • Shaw et al., 2018 • Learn embeddings for relative positions: 𝑖 − 𝑗 = -max_dist, …, -1, 0, 1, …, max_dist • They use max_dist = 16 in the paper • Distance to between words at positions 𝑖 and 𝑗: 𝑖 − 𝑗 = −3 • Can’t add them to input word embeddings: A different embedding has to be used for each output position → Embeddings are added during attention computation instead 57 𝑤𝑖 𝑤𝑖−1 𝑤𝑖−2 𝑤𝑗 𝑤𝑖+1 Relative Position Embedding • Standard attention function: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖𝑘𝑗 ⊤ 𝑑 • Attention function with relative position embeddings pos𝑖−𝑗: 𝑓att 𝑞𝑖, 𝑘𝑗 = 𝑞𝑖 𝑘𝑗 + pos𝑖−𝑗 ⊤ 𝑑 • Shaw et al., 2018 has another relative position term for distance of queries to values, but that one doesn’t help 58 Rotary Position Embedding (RoPE) • Su et al., 2024 (initial version from 2021) • Relative position embedding used in modern LLMs • Multiplies queries and keys with rotation matrix • Different rotation matrix for every position • Multiplying two rotation matrices will give the rotation matrix of the relative position difference • Works with linear attention variants • Inner product decreases with larger distances 59Su et al., 2024","libVersion":"0.3.2","langs":""}