\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\title{}


% Iterate through the authors except last to add \And. 

\author{%
}

% \author{%
%   David S.~Hippocampus \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }


\begin{document}


\maketitle


\begin{abstract}
    Add your abstract at the beginning of your markdown file like this 
  \begin{verbatim}
  --- 
  title: "Your Title" 
  abstract: "your abstract here"
  authors:
  - name: Leonardo V. Castorina
    affiliation: School of Informatics
    institution: University of Edinburgh
    email: justanemail@domain.ext
    address: Edinburgh
  - name: Coauthor
    affiliation: Affiliation
    institution: Institution
    email: coauthor@example.com
    address: Address
  ---
  \end{verbatim}
  This is called YAML frontmatter. If you set your abstract correctly you should not see this message.
  \end{abstract}


\section{Lost \& Found: Predicting Locations from
Images}\label{lost-found-predicting-locations-from-images}

Team name: ExampleTeam

\textbf{Group Members} \href{https://gitlab.com/Killusions}{Linus
Schlumberger} \href{https://gitlab.com/Valairaa}{Lukas St√∂ckli}
\href{https://gitlab.com/yusigrist}{Yutaro Sigrist}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{title: \# Table of content}
\NormalTok{style: nestedList}
\NormalTok{includeLinks: true}
\end{Highlighting}
\end{Shaded}

\section{Introduction}\label{introduction}

\subsection{Problem description}\label{problem-description}

Nowadays, images are often automatically enriched with various data from
different sensors within devices, including location metadata. However,
this metadata often gets lost when images are sent through multiple
applications or when devices are set not to track locations for privacy
reasons. As a result, images may initially have metadata, but it is lost
when shared with friends or published online. This raises the question:
Is it possible to re-enrich these images with their location after the
metadata is lost?

The main goal of this project is to determine if an Image Classification
Model can outperform humans in guessing the countries or regions of
images based solely on images with low resolution and no additional
information. \#\#\# Project Overview This project explores the
development of an Image Classification model, focusing on simple
street-view images grouped by countries to predict the country where an
image was taken. Given limited prior experience with Image
Classification, this initiative aims to enhance understanding and skills
in this domain. The first objective is to create a model capable of
identifying the country from a given image. Building upon this, a second
model will be developed to predict the exact region of the image,
providing a more precise location than just the country.

The main goal is to develop a robust Image Classification model that can
serve as a foundational tool for various applications. This overarching
objective supports the specific sub-goals of predicting the country and
coordinates of an image. This leads to the question: for what main
purposes could an image classifier for countries or coordinates be
valuable? By exploring potential applications, the project aims to
demonstrate the broader utility of the developed models in real-world
scenarios. \#\#\# Potential Applications - \textbf{Helping find missing
persons}: Our solution can help find where missing people might be by
analyzing pictures shared publicly. The emotional impact of helping
reunite families or providing important clues is huge. Especially when
the model will be used in addition to the search process for the police.
For missing people, every second counts after a kidnapping, especially
when the search is international. - \textbf{Rediscovering memories and
family history}: Have you ever come across an old image of someone close
to you? Maybe of a deceased family member or someone who may just not
remember where it was taken. Our model can try to predict the rough
location to help you rediscover your past. - \textbf{Supporting
humanitarian action:} In disaster situations, it could help to quickly
identify the most affected areas by analyzing current images from social
media or aid organizations. This would improve the coordination of
rescue and relief efforts and offer hope and support to those impacted.
- \textbf{Discovering new travel destinations:} Have you ever
encountered stunning images of places on Instagram or other social media
platforms and wondered where they were taken? Our image classifier can
help you with that. By analyzing the image, our classifier can identify
the location and provide you with the information you need to plan your
next visit to this amazing place. This way, you can discover new and
exciting travel destinations that you may have never known about before.
- \textbf{Classification as a service}: With this service, we will help
other companies or data science projects label their data. Sometimes
companies want to block, permit, or deploy individual versions of their
applications in different countries. Some countries have more
restrictions for deploying applications, therefore the image predictor
can help the companies have the right version on the right devices for
these countries.

\subsection{State of the Art}\label{state-of-the-art}

Recent advancements in deep learning have significantly enhanced the
ability to determine the geographical location of an image. DeepGeo,
developed by Suresh et al., leverages visual cues such as vegetation and
man-made structures like roads and signage to infer locations. This
approach aims to replicate the human ability to use environmental
indicators and prior knowledge for geolocation (Suresh et al., 2018).
The DeepGeo model restricts its scope to the United States, utilizing
panoramic viewpoints to classify images based on their state. Each input
sample consists of four images taken at the same location, oriented in
cardinal directions, which are then classified into one of 50 state
labels (Suresh et al., 2018).

In contrast, PlaNet, developed by Weyand et al., tackles global image
geolocation. It employs a deep convolutional neural network based on the
Inception architecture, trained on 126 million geotagged photos from
Flickr. PlaNet's method involves partitioning the world map into
multi-scale geographic cells and classifying test images into these
cells. Despite its large dataset and extensive training, PlaNet achieves
a country-level accuracy of only 30\% on its test set (Weyand et al.,
2016). M2GPS, developed by Hays and Efros, is another significant
baseline in scalable image geolocation. This model performs data-driven
localization by computing the closest match via scene matching with a
large corpus of 6 million geotagged Flickr images, utilizing features
such as color and geometric information. IM2GPS's approach demonstrates
the importance of leveraging large datasets for effective geolocation
(Hays and Efros, 2008)\hspace{0pt}\hspace{0pt}.

Banerjee's work emphasizes the classification task of predicting image
location solely based on pixel data. Their research highlights the use
of CNNs and transfer learning to achieve high-accuracy models capable of
superhuman performance. CNNs are particularly effective due to their
ability to capture low-level and complex spatial patterns (Banerjee,
2023). Dayton et al.~explored a similar task by using a ResNet-50 CNN
pre-trained on ImageNet for classifying street view images from the game
GeoGuessr. Their model utilized transfer learning to refine the
pre-trained network on a dataset specifically curated for the task,
resizing images to 224x224 pixels for input. By fine-tuning the last
layers of ResNet-50, they achieved a test accuracy of over 70\% for 20
different countries, highlighting the efficacy of leveraging pre-trained
models for geolocation tasks (Dayton et al., 2024).

Another notable model is PIGEON, which combines semantic geocell
creation with multi-task contrastive pretraining and a novel loss
function. PIGEON is trained on GeoGuessr data and demonstrates the
capability to place over 40\% of its guesses within 25 kilometers of the
target location globally, which is remarkable. This model highlights the
importance of using diverse datasets and innovative training techniques
to enhance geolocation accuracy (Haas et al., 2024). While these models
exhibit high accuracy in controlled conditions, they often rely on
high-resolution images, multiple perspectives, and enriched datasets
that do not reflect real-world scenarios. For instance, DeepGeo's use of
panoramic images and PlaNet's extensive dataset of geotagged Flickr
photos introduce biases towards urban areas and well-known landmarks,
limiting their effectiveness in arbitrary or rural locations (Suresh et
al., 2018). Additionally, these models struggle to generalize to
lower-resolution images and more diverse datasets that include unseen
locations, as highlighted by the performance discrepancies observed in
models like PIGEON when applied to varied datasets (Haas et al., 2024).

Furthermore, Banerjee's research on digital image classification since
the 1970s underscores the evolution from using textural and colour
features to the current reliance on CNNs. This historical perspective
reveals that early models had limited discriminative power and
robustness, which were significantly improved with the advent of SIFT
(Scale-invariant feature transform) and visual Bag-of-Words models.
However, the transition to CNNs marked a pivotal shift due to their
superior ability to capture both low-level and high-level features
(Banerjee, 2023). Dayton et al.~further illustrate the application of
transfer learning in geolocation by refining a pre-trained ResNet-50
model on a specific geolocation task. Their work highlights the
importance of data augmentation and hyperparameter tuning in improving
model performance, as well as the need for balanced datasets to reduce
bias and enhance generalizability (Dayton et al., 2024).

To develop more robust and universally applicable geolocation models, it
is essential to focus on creating systems that can operate effectively
with lower-resolution images and without the need for panoramic views or
extensive enriched datasets. This involves training models on diverse,
real-world datasets that include a variety of image types, from urban
streets to rural landscapes, captured under different conditions and
perspectives. By doing so, the models can better mimic the conditions
under which humans typically use images for geolocation, such as in
social media posts, emergency situations, or historical photo analysis.
For instance, PIGEOTTO, an evolution of PIGEON, takes a single image per
location and is trained on a larger, highly diverse dataset of over 4
million photos from Flickr and Wikipedia, excluding Street View data.
This approach demonstrates the model's ability to generalize to unseen
places and perform well in realistic scenarios without the need for
multiple images per location (Haas et al., 2024). \#\# Contributions

This paper has four main contributions. Firstly, we address the
limitations of current geolocation models by developing a novel approach
that leverages low-resolution images, enabling accurate geolocation in
more realistic and diverse scenarios. Secondly, we enhance the dataset
by expanding it to include more countries, ensuring a balanced and
distributed representation, which is crucial for mitigating biases
present in state-of-the-art models. Thirdly, we tackle hardware
limitations by optimizing image sizes, making the model more accessible
and efficient for deployment on various hardware platforms. Finally, we
propose a new methodology for training and fine-tuning our model,
incorporating the latest advancements in transfer learning and data
augmentation techniques, which significantly improve the model's
performance and generalizability across different real-world
applications.

Our contributions aim to advance the field of image geolocation, making
it more practical and effective for a wide range of applications, from
aiding in disaster response to rediscovering family histories and
beyond. By addressing these key challenges, we believe our work will
pave the way for the development of more robust and universally
applicable geolocation technologies.

\section{Methods (REPRODUCIBILIY is the main
goal)}\label{methods-reproducibiliy-is-the-main-goal}

\subsection{Data collection}\label{data-collection}

\subsubsection{Data source}\label{data-source}

When it comes to relatively uniform street imagery, there are not many
sources. Google Street View \textless-LINK\textgreater{} being by far
the biggest. But instead of sourcing our images directly from Google, we
wanted to have a more representative distribution, as well as a more
interactive demonstration.

For this reason we instead opted for the online Geography game called
Geoguessr \textless-LINK\textgreater. This has the advantage of not
manually having to source where there is coverage, at what density and
decide on a distribution. The game revolves around being ``dropped''
into a random location on Google Street View, and having to guess where
it is located.

\textless-POTENTIALLY INSERT GEOGUESSR PICTURE\textgreater{}

Originally the player is allowed to move around, but there are modified
modes to create harder difficulties which prevent the moving or even the
panning of the camera, which is what we'll be opting for. This will also
allow it to generalize more to other static pictures than if we were
using the 360¬∞ spheres.

\textless-POTENTIALLY INSERT SAMPLE PICTURES\textgreater{}

Because different countries are of different sizes, but also have
different amounts of Google Street View coverage, deciding on a
representative distribution for generalization would be very difficult.
Instead, we opted to play the Geoguessr multiplayer game mode called
``Battle Royale: Countries'' \textless-LINK\textgreater. This game mode
revolves around trying to guess the country of a location before the
opponents do. It has a much more even distribution of countries, while
still taking into account the densities of different places.

\textless-INSERT MULTIPLAYER GRAPH\textgreater{}

Unfortunately, data collection using a multiplayer game mode is quite
slow, as even though we do not need to guess and can spectate the rest
of the game, we still need to wait for the other players to guess every
round. The number of concurrent games was also be limited by the number
of currently active players. Additionally, while spectating it is not
easily possible to get the exact coordinates of a location, restricting
us to only predicting the correct countries. Lastly, we were detected by
their anti-cheating software as the automation environment is injecting
scripts into the website.

Instead, we chose to collect data through the most popular singleplayer
game mode called ``World'' (``Classic Maps''), by putting in arbitrary
guesses and playing a lot of rounds. This allowed us to collect data a
lot quicker, as well as also collecting the coordinates, however, it
came at the cost of a very skewed distribution.

\textless-INSERT SINGLEPLAYER GRAPH\textgreater{}

To remedy this, we instead use the country distribution of our
multiplayer games and apply it to our collected singleplayer data. This
leaves a lot of data unused and forces us to remove very rare countries,
but it allows us to get the required amount of data a lot quicker.

\textless-POTENTIALLY INSERT MAPPED SINGLEPLAYER GRAPH\textgreater{}

\subsubsection{Web scraping}\label{web-scraping}

To collect this data we built our own scraper, utilizing the testing and
browser automation framework ``Playwright'' \textless-LINK\textgreater.
We then deployed 5 parallel instances of this script to a server and
periodically retrieved the newly collect data.

Our script starts off by logging and and storing the cookies for further
sessions, it then accepts the cookie conditions and attempts to start a
game. We do this by navigating the page using the text, as there are no
stable identifiers. For multiplayer it additionally checks for
rate-limiting or if it joined the same game as another instance of the
script, it those cases it waits for a certain amount of time and
attempts the same again.

After a game started it will wait for a round to start, wait for the
image to load, hide all other elements on the page and move the mouse
cursor out of the way and take a screenshot. For singleplayer it then
guesses a random location while in multiplayer it waits for the round to
end, spectating the rest of the game afterwards. At the end of each
round the coordinates or in the case of multiplayer the country are read
from the page and saved to a file. Both these files are named after the
``game id'' we extract from the URL, preventing duplicates. This is the
repeated until the script is stopped.

\textless-POTENTIALLY INSERT SCRAPING CONTROL FLOW GRAPH\textgreater{}

Initially we had a lot of issues with stability, especially with our
parallelized workers. After we got rid of hardware bottlenecks we also
looked to eliminate as many fixed waits as possible, replacing them wait
dynamic ones to avoid timing issues. Finally, we made sure to enable
auto-restarting and added a lot of other measures to completely restart
after our environment stops working, which can happen during extended
scraping sessions. We then let this script run in parallel, non-stop for
multiple weeks, collecting \textless-INSERT FIGURE\textgreater{}
multiplayer datapoints and \textless-INSERT FIGURE\textgreater{}
singleplayer datapoints.

To make sure our data is collected correctly, we manually inspected it
periodically. Any faults we noticed in the images like black screens and
blurring, we would address later in our filtering. However, we also had
to inspect whether the coordinates and countries were accurate.

(After an initial run of our singleplayer script, we noticed that the
way we collected coordinates in multiplayer did no longer work and had
been collecting incorrect coordinates for tens of thousands of images.
To address this, we built an additional script looking up the correct
coordinates using the ``game id'', this was a lot quicker than the
collection of new data, allowing us to correct the mistake quite
quickly. We also then used this new way of looking up coordinates for
our collection script.) \#\# Data processing

\subsubsection{Resizing of the images}\label{resizing-of-the-images}

We can't train the classifier using images in a high resolution, because
our resources are limited, and also often images (like from missing
persons) are also very low quality. So we decided to reduce the
resolution, at the beginning of the processing, about the 1/4 of the
original resolution of 1280p x 720p. This also helps to move the images
for learning to the server or also between us and also loading takes lot
less time for future processing steps.

\subsubsection{Enriching (Singleplayer coordinates, Multiplayer names)
-\textgreater{}
ls}\label{enriching-singleplayer-coordinates-multiplayer-names---ls}

Follows\ldots{}

(Issues with reverse geocoding, country name matching)

\subsubsection{Region Enriching (Source,
Mapping)}\label{region-enriching-source-mapping}

So to predict the Region of the image, we first searched for a list of
regions around the world. And decide to use the geojson file from
Natural Earth. Since for each region we had a list of coordinates, which
marks the border of the region, we had to get the middle point of each
one. Where the python library ``geopands'' comes in handy. This library
has the advantage to be able to work with geojson files and to have an
integrated middle point calculation function. In addition, we add a
unique region\_name for each region using the name of the region +
country name + id. This is needed since some region names have similar
or the same name. After this preparation, we used the middle point to
get the region for each image using their coordinates using k-nearest
neighbor method. \#\#\# Mapping to a distribution

As mentioned in the previous section (Web scraping), our singleplayer
data is skewed towards a few countries, with some countries only
appearing very rarely. To address this, we are mapping our singleplayer
data to the country distribution of our multiplayer data. This allows us
to have a better distribution while still not having every country
appear with the same frequency to account for size and coverage
differences. It, however, comes with the downside of not being able to
use all of our data, although some tests showed that using all of our
data unmapped performed worse \textless-CHECK AND MENTION
RESULTS\textgreater.

Unfortunately, this also doesn't allow us to include all countries as
some of them do not appear often enough and would reduce the number of
images we are allowed to use for other countries as well. To achieve a
mapping including enough files while including as many countries as
possible, we set a minimum threshold of how often a country has to
appear within the singleplayer data (\textless-INSERT
FIGURE\textgreater). Because this included too few countries, we added a
slack factor (\textless-INSERT FIGURE\textgreater), allowing countries
that could almost meet the distribution to be included as well.

Finally, we saved this as a list of file names using our
``data-loader'', and commit it to our repository, making our runs
reproducible. We created a few different variants of the mapped list,
sometimes including more countries and other times more files per
country, until we found a good balance.

\textless-POTENTIALLY INSERT MAPPED SINGLEPLAYER GRAPH\textgreater{}

\subsubsection{Filtering of data}\label{filtering-of-data}

To address issues with our scraping's inherently unstable nature, as
well as the big variety of Google Street View images, we had to do some
automated filtering of unsuitable data. This consisted of both filtering
our images, but also the corresponding data. After filtering we again
saved this as a list of file names using our ``data-loader'', and commit
it to our repository.

To filter images we started by setting a minimum threshold of the
biggest variance of color between the pictures of an image, meaning
either red, green or blue has to vary by some amount. This easily
filters out black screens and dark images, like the ones indoor or
inside tunnels. Additionally, we added a threshold for the variance
after the laplacian kernel was applied, allowing us to filter some
blurry and low quality images. We set our thresholds after doing manual
sampling and some test runs.

\textless-INSERT SAMPLE PICTURES WITH VARIANCE\textgreater{}

Additionally, we realized that some rounds were in the exact same
locations, so we decided to filter out duplicates by comparing the
coordinates, only keeping the first image. This, as well as the image
filtering, comes with the added benefit of filtering corrupted data,
which would otherwise have to be handled in our training code.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Requirements:} 1. \textbf{Core Competency in Data Science:} Data
processing is a fundamental step in any data science project.
Demonstrating this process shows the student's ability to handle and
prepare data for analysis, which is a critical skill in the field. 2.
\textbf{Transparency and Reproducibility:} Detailing the data processing
steps ensures transparency and aids in the reproducibility of the
results, which are key aspects of scientific research. \_\_\_

To talk about:

Basic method (Cross-entropy, \ldots)

(Coordinates attempt)

\subsection{Regions with custom loss}\label{regions-with-custom-loss}

For the region-prediction we use a custom loss function. Which, in short
text, is a loss function not only look if the correct region is
predicted, it considers also the distance to the correct coordinates.
Which means if the predicted region is only slightly off then the loss
is not that big like if it is far off. There is the paper ``PIGEON:
Predicting Image Geolocations'' from Stanford University, which comes in
handy for this task. They're using the haversine smooth loss function.
(Haas et al., 2024).

\subparagraph{The steps of the custom loss
function}\label{the-steps-of-the-custom-loss-function}

The haversine distance is a measure of the shortest distance between two
points on the surface of a sphere, given their longitudes and latitudes.
It is calculated using the following formula:

\[
\text{Hav}(\mathbf{p_1}, \mathbf{p_2}) = 2r \arcsin \left( \sqrt{\sin^2 \left( \frac{\phi_2 - \phi_1}{2} \right) + \cos(\phi_1) \cos(\phi_2) \sin^2 \left( \frac{\lambda_2 - \lambda_1}{2} \right)} \right) 
\]

where

\begin{itemize}
\tightlist
\item
  \(r\) is the radius of the Earth (6371 km in this implementation),
\item
  \(\mathbf{p_1}, \mathbf{p_2}\) are the 2 points with longitude
  \(\lambda\) and latitude \(\phi\)
\end{itemize}

The smoothed labels are calculated using the following formula:

\[
y_{n,i} = \exp \left( - \frac{\text{Hav}(\mathbf{g_i}, \mathbf{x_n}) - \text{Hav}(\mathbf{g_n}, \mathbf{x_n})}{\tau} \right)
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{g_i}\)\hspace{0pt} are the centroid coordinates of the
  geocell polygon of cell \(\mathbf{i}\)
\item
  \(\mathbf{g_n}\)\hspace{0pt} are the centroid coordinates of the true
  geocell.
\item
  \(\mathbf{x_n}\)\hspace{0pt} are the true coordinates of the example.
\item
  \(\tau\) is a temperature parameter.
\end{itemize}

Finally, the cross-entropy loss is calculated between the model outputs
and the smoothed labels.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Requirements:} 1. \textbf{Understanding and Application:} This
section allows students to demonstrate their understanding of various
methodologies and their ability to apply appropriate techniques to their
specific project. 2. \textbf{Rationale and Justification:} Discussing
the methods used provides insight into the student's decision-making
process and the rationale behind choosing specific approaches. \_\_\_
\#\# Transfer Learning

\subsubsection{Model architectures -\textgreater{}
ls}\label{model-architectures---ls}

Follows\ldots{} also take the literature from the sources! Without them,
we just have 6 and need 7 in total.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Sources: {[}5{]}: \href{https://arxiv.org/abs/1905.11946}{EfficientNet
Rethinking Model Scaling for CNNs} {[}6{]}:
\href{https://arxiv.org/abs/1801.04381}{MobileNetV2: Inverted Residuals
and Linear Bottlenecks} {[}7{]}:
\href{https://arxiv.org/abs/1512.03385}{Deep Residual Learning for Image
Recognition / ResNet} \_\_\_ \#\#\# Data augmentation -\textgreater{} ls
\#\#\# Hyperparameter tuning -\textgreater{} ls \#\# Human baseline
performance

\subsubsection{Collection of baseline
scores}\label{collection-of-baseline-scores}

To compare our model to the performance of a human classifier, we would
first have to measure the performance of a similar human. To calculate
this, we built a small interactive application using ``Gradio''
\textless-LINK\textgreater. It loads a random image in our downscaled
resolution, though not quite as low as most of our models are trained
on, and asks the user to type in the 5 most likely countries. This then
allows us to calculate a reasonable Top-1, Top-3 and Top-5 accuracy for
comparison with our model.

Follows\ldots{}

\subsection{Machine Learning Operations
(MLOps)}\label{machine-learning-operations-mlops}

\subsubsection{Project structure}\label{project-structure}

As we did for our last project (``DSPRO1''), we are using a ``monorepo''
setup with a pipeline-style setup consisting of numbered folder and
subfolders, each representing different stages and sub-stages of our
dataflow, from data collection to model training. Every stage consists
of at least one Jupyter Notebook, with more helpers and reused python
code dispersed throughout the project. Each notebook saves the generated
data in its current folder, making the flow obvious. Within each
sub-step, the notebooks can be run in arbitrary order because they are
not inter-dependent.

\subsubsection{Handling a lot of files}\label{handling-a-lot-of-files}

Differing from our last project, however, is the amount of data. With
our scraping generating hundreds of thousands of images, we could not
store them in our git repository. Instead, we opted for storing them in
our server we had used for scraping, although in a scaled and already
enriched format, making it quicker to get our training and repository up
and running on a new machine. This server is public to allow for our
results to be reproduced.

Using a server for storage made storing the files easy, but it came with
the added challenge of reproducibility. Ideally, we would want to store
all of our data on the server but only pull the required ones for a
particular training, ensuring that they were always the same ones.

(To quickly return a list of all files present without overloading the
web server we use to serve the files, we wrote a small PHP script
returning the files names as a list of links, which can be easily
parsed.)

To solve this came up a custom set of helpers called ``data-loader''.
This would get the list of files from our server, filter them by
criteria, sort, optionally shuffle or limit them, and output the full
paths to the files that should be used for this processing step or
training. Note that each data point consists of both an image file and a
JSON file, the ``data-loader'' treats them as pairs and has stable
shuffling and limiting behavior, no matter where or how the files are
stored.

Behind the scenes, it writes a text file (``data-list'') to the
repository listing all of the files used. This file is meant to be
committed to the repository and ensures that all future runs of this
setup will get the exact same files, otherwise throw an error. If some
files were still missing locally, they are automatically downloaded
before returning the paths.

Once we had this running, we could easily deploy this on persistent
cloud environments like HSLU's GPUHub, however, we also wanted to be
able to deploy it on Google Colab \textless-LINK\textgreater. which does
not have persistent storage. To address this, we wrote a shell script
automatically clone our git repository from GitLab
\textless-LINK\textgreater, install dependencies using ``Poetry''
\textless-LINK\textgreater, convert the training notbooking to plain
python and run it.

(Even with the script, setup was still slow because hundreds of
thousands of files had to be downloaded from our server first. To solve
this, we mounted a Google Drive \textless-LINK\textgreater{} and stored
our files there. However, since the drive adapter is slow and seizes to
work with a lot of files, we had to take a couple of measures to address
this.

Firstly, we stored our downloaded files in nested directories,
containing the first and second characters in the ``game ids'' of the
files. Secondly, we store a list of all files present in the Google
Drive, preventing a slow file listing, and lastly, we store the files in
a zip file, copy the entire file and uncompress them on the local
storage of the runner. This allowed us to quickly deploy our model
training to Google Colab, which gave us the chance to rain on more
powerful GPUs.)

To speed up training in other environments, especially when using a lot
of transformations for data augmentation, we cache the prepared dataset
using pytorch right before training. The dataset is saved to a file
named after the preprocessing parameters, as well as a hash of all file
names to ensure consistency. A file only containing the test data after
the split is also saved to make calculating the metrics quicker.

For monitoring and deploying we log and push all of our run data to
``Weights and Biases'' \textless-LINK\textgreater, which allows us to
plot and compare many runs, as well as automatically do
hyperparameter-tuning. After each training we also push the model
weights as well as the test data, if it has not been saved before,
otherwise a link to it. This allows us to deploy a model and calculate
the final metrics in seconds.

To talk about:

Creating the demo for the geoguessr wizard and how we are deploying the
model in this real-world scenario

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Requirements:} 1. Practical Application: This section emphasizes
the practical aspect of machine learning. It's not just about building
models but also about deploying them effectively in real-world
scenarios. 2. Bridging Theory and Practice: It allows students to
demonstrate their ability to translate theoretical knowledge into
practical applications, showcasing their readiness for industry
challenges. \_\_\_

\subsection{Model performance on other
datasets}\label{model-performance-on-other-datasets}

Follows\ldots{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Requirements:} 1. Ensuring Model Reliability: Model validation
is crucial for assessing the accuracy and reliability of the model. This
section shows how the student evaluates the performance and
generalizability of their model. 2. Critical Evaluation: It encourages
students to critically evaluate their model's performance, understand
its limitations, and discuss potential improvements. \_\_\_

\section{Experiments and Results (and also
discussions)}\label{experiments-and-results-and-also-discussions}

\subsection{Predicting countries}\label{predicting-countries}

Follows\ldots{}

\subsection{Predicting regions}\label{predicting-regions}

Follows\ldots{} \_\_\_

List here all results in plots, confusion matrix and so on. The goal of
these sections is to present our result and explain how they help in
solving the problem we are working on, and to answer the research
questions we are trying to answer.

Our Hypothesis: The main goal of this student project is to determine if
an Image Classification Model can outperform humans in guessing the
countries or regions of images based solely on the image itself, without
additional information. \_\_\_

\section{Conclusions and Future Work}\label{conclusions-and-future-work}

Follows\ldots{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Here we should discuss the implications of our results, our limitations,
and possible further research possibilities. We should be very honest
especially about limitations. \_\_\_ \# References

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Suresh, Sudharshan, Nathaniel Chodosh, and Montiel Abello. ``DeepGeo:
  Photo Localization with Deep Neural Network.'' arXiv, February 16,
  2016. https://arxiv.org/abs/1810.03077.
\item
  Weyand, T., Kostrikov, I., \& Philbin, J. ``PlaNet - Photo Geolocation
  with Convolutional Neural Networks.'' arXiv, October 6, 2018.
  https://arxiv.org/abs/1602.05314.
\item
  James Hays, Alexei A. Efros. ``IM2GPS: estimating geographic
  information from a single image. Proceedings of the IEEE Conf. on
  Computer Vision and Pattern Recognition (CVPR)''. graphics, n.d.,
  2008. http://graphics.cs.cmu.edu/projects/im2gps/.
\item
  Banerjee, Arsh. ``Image Geolocation with Computer Vision,''
  arshbanerjee, May 9, 2023.
  http://www.arshbanerjee.com/uploads/paper/3bda9\_20230723185809.pdf
\item
  Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ``PIGEON:
  Predicting Image Geolocations.'' arXiv, May 28, 2024.
  https://arxiv.org/abs/2307.05845.
\item
  Dayton, Finn, Jeffrey Heo, and Eric Werner. ``CNN Plays Geoguessr:
  Transfer Learning on ResNet50 for Classifying Street View Images,''
  cs229, n.d., 2023. https://www.finndayton.com/CS229\_Final\_Report.pdf
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}